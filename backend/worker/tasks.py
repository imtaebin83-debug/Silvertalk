"""
Celery AI ì²˜ë¦¬ íƒœìŠ¤í¬ (TTS ì œê±° ë²„ì „)
- ë™ì  ë””ë°”ì´ìŠ¤ ê°ì§€ (CUDA/CPU ìë™ ì „í™˜)
- AI ëª¨ë¸: Faster-Whisper (STT), Gemini (LLM)
- TTSëŠ” í´ë¼ì´ì–¸íŠ¸ expo-speechë¡œ ëŒ€ì²´
- CUDA ì¶©ëŒ ë°©ì§€: Lazy ì´ˆê¸°í™” + ëª…ì‹œì  .env ë¡œë“œ
"""

# ============================================================
# ìµœìš°ì„ : .env ë¡œë“œ (GEMINI_API_KEY ë“±)
# ============================================================
from dotenv import load_dotenv
load_dotenv()

import os
import logging
import traceback
import soundfile as sf
from celery import Task
from worker.celery_app import celery_app
from common.config import settings
from common.image_utils import preprocess_image_for_ai, preprocess_image_file, ImageProcessingError

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ============================================================
# Type Safety Layer: ì‘ë‹µ í¬ë§· ê²€ì¦ í—¬í¼
# ============================================================
def format_response(required_keys: list, data: dict, schema_name: str = "UnknownSchema") -> dict:
    """
    Celery íƒœìŠ¤í¬ ë°˜í™˜ê°’ ê²€ì¦ ë° í¬ë§·íŒ…
    
    Args:
        required_keys: í•„ìˆ˜ í•„ë“œ ëª©ë¡
        data: ë°˜í™˜í•  ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        schema_name: ìŠ¤í‚¤ë§ˆ ì´ë¦„ (ë””ë²„ê¹…ìš©)
    
    Returns:
        dict: ê²€ì¦ëœ ì‘ë‹µ ë”•ì…”ë„ˆë¦¬
    
    Raises:
        ValueError: í•„ìˆ˜ í‚¤ê°€ ì—†ëŠ” ê²½ìš°
    """
    # ëª¨ë“  UUID í•„ë“œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    for key in ["session_id", "user_id", "video_id", "task_id"]:
        if key in data and data[key] is not None:
            data[key] = str(data[key])
    
    # statusëŠ” ì†Œë¬¸ìë¡œ í†µì¼
    if "status" in data:
        data["status"] = str(data["status"]).lower()
    
    # í•„ìˆ˜ í‚¤ ê²€ì¦
    missing_keys = [k for k in required_keys if k not in data]
    if missing_keys:
        logger.warning(f"[{schema_name}] ëˆ„ë½ëœ í•„ë“œ: {missing_keys}")
        # ëˆ„ë½ëœ í•„ë“œì— ê¸°ë³¸ê°’ ì„¤ì •
        for key in missing_keys:
            if key == "status":
                data[key] = "error"
            elif key == "message":
                data[key] = "Unknown error"
            else:
                data[key] = ""
    
    logger.debug(f"[{schema_name}] ê²€ì¦ ì™„ë£Œ: {list(data.keys())}")
    return data

# ============================================================
# ë™ì  ë””ë°”ì´ìŠ¤ ê°ì§€ (Lazy ì´ˆê¸°í™”)
# ============================================================
_device_cache = None
DEVICE = None
COMPUTE_TYPE = None

def detect_device():
    """
    ì‹¤í–‰ í™˜ê²½ì— ë”°ë¼ ìë™ìœ¼ë¡œ ë””ë°”ì´ìŠ¤ ì„ íƒ
    Worker fork ì´í›„ì— í˜¸ì¶œë˜ì–´ì•¼ CUDA ì¶©ëŒ ë°©ì§€
    
    Returns:
        tuple: (device, compute_type)
    """
    global _device_cache, DEVICE, COMPUTE_TYPE
    
    if _device_cache is not None:
        return _device_cache
    
    # torchëŠ” í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ import (Worker fork ì´í›„)
    try:
        import torch
        
        if torch.cuda.is_available():
            device = "cuda"
            compute_type = "float16"  # Whisperìš©
            gpu_name = torch.cuda.get_device_name(0)
            logger.info(f"ğŸš€ GPU ê°ì§€: {gpu_name} - CUDA ëª¨ë“œ í™œì„±í™”")
        else:
            device = "cpu"
            compute_type = "int8"
            logger.info("ğŸ’» GPU ë¯¸ê°ì§€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        
        _device_cache = (device, compute_type)
        DEVICE = device
        COMPUTE_TYPE = compute_type
        return _device_cache
    
    except Exception as e:
        logger.error(f"âŒ ë””ë°”ì´ìŠ¤ ê°ì§€ ì‹¤íŒ¨: {str(e)}, CPUë¡œ fallback")
        _device_cache = ("cpu", "int8")
        DEVICE = "cpu"
        COMPUTE_TYPE = "int8"
        return _device_cache


# ============================================================
# AI ëª¨ë¸ ì „ì—­ ë³€ìˆ˜ (ì›Œì»¤ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ë¡œë“œ)
# ============================================================
whisper_model = None
# tts_model = None  # TTS ì œê±° (ì‹œê°„ ì ˆì•½)
gemini_model = None


# ============================================================
# ëª¨ë¸ ë¡œë”© í•¨ìˆ˜
# ============================================================
def load_models():
    """
    AI ëª¨ë¸ ì´ˆê¸°í™” (ì›Œì»¤ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰)
    
    ì—ëŸ¬ ì²˜ë¦¬:
    - ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì‹œì—ë„ ì›Œì»¤ê°€ ì£½ì§€ ì•Šë„ë¡ try-except ì‚¬ìš©
    """
    global whisper_model, tts_model, gemini_model
    
    # ë””ë°”ì´ìŠ¤ ê°ì§€ (ì²« ì‹¤í–‰)
    device, compute_type = detect_device()
    logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤ ì„¤ì •: device={device}, compute_type={compute_type}")
    
    # STT: Faster-Whisper ë¡œë”©
    if whisper_model is None:
        try:
            from faster_whisper import WhisperModel
            
            # RunPod Volume ê²½ë¡œ ì‚¬ìš© (ì˜êµ¬ ì €ì¥)
            whisper_root = os.path.join(settings.models_root, "whisper")
            os.makedirs(whisper_root, exist_ok=True)
            
            logger.info(f"[Whisper] ëª¨ë¸ ë¡œë”© ì‹œì‘... (ê²½ë¡œ: {whisper_root})")
            
            # CUDA ì‹œë„ (cuDNN + libcublas ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ì „ ì²´í¬)
            if device == "cuda":
                try:
                    # CUDA í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ì „ ì²´í¬
                    import ctypes
                    
                    # 1. cuDNN ì²´í¬
                    try:
                        ctypes.CDLL("libcudnn_ops_infer.so.8")
                        logger.info("âœ… cuDNN ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸ ì™„ë£Œ")
                    except OSError as e:
                        raise Exception(f"cuDNN ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½: {e}")
                    
                    # 2. libcublas ì²´í¬ (CUDA 11.x ë˜ëŠ” 12.x)
                    cublas_found = False
                    cublas_version = None
                    for cublas_ver in ["libcublas.so.12", "libcublas.so.11"]:
                        try:
                            ctypes.CDLL(cublas_ver)
                            logger.info(f"âœ… {cublas_ver} í™•ì¸ ì™„ë£Œ")
                            cublas_found = True
                            cublas_version = cublas_ver
                            break
                        except OSError:
                            continue
                    
                    if not cublas_found:
                        raise Exception("libcublas ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½ (11 ë˜ëŠ” 12 ë²„ì „ í•„ìš”)")
                    
                    # libcublas.so.12ê°€ í•„ìš”í•œë° .11ë§Œ ìˆëŠ” ê²½ìš° ê²½ê³ 
                    if cublas_version == "libcublas.so.11":
                        logger.warning("âš ï¸ CTranslate2ê°€ libcublas.so.12ë¥¼ ìš”êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
                        logger.warning("âš ï¸ ì‹¤íŒ¨ ì‹œ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±: ln -sf libcublas.so.11 libcublas.so.12")
                    
                    # ë¼ì´ë¸ŒëŸ¬ë¦¬ ì²´í¬ í†µê³¼ â†’ Whisper ëª¨ë¸ ë¡œë”©
                    whisper_model = WhisperModel(
                        model_size_or_path="medium",
                        device=device,
                        compute_type=compute_type,
                        download_root=whisper_root
                    )
                    logger.info(f"âœ… Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ (model=medium, device={device}, path={whisper_root})")
                
                except Exception as cuda_error:
                    logger.warning(f"âš ï¸ CUDA ë¼ì´ë¸ŒëŸ¬ë¦¬ ì²´í¬ ì‹¤íŒ¨: {cuda_error}")
                    logger.warning("âš ï¸ CPU ëª¨ë“œë¡œ ê°•ì œ ì „í™˜")
                    whisper_model = WhisperModel(
                        model_size_or_path="medium",
                        device="cpu",
                        compute_type="int8",
                        download_root=whisper_root
                    )
                    logger.info(f"âœ… Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ (model=medium, device=cpu, path={whisper_root})")
            
            else:
                # CPU ëª¨ë“œ ì§ì ‘ ë¡œë”©
                whisper_model = WhisperModel(
                    model_size_or_path="medium",
                    device="cpu",
                    compute_type="int8",
                    download_root=whisper_root
                )
                logger.info(f"âœ… Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ (model=medium, device=cpu, path={whisper_root})")
        except Exception as e:
            logger.error(f"âŒ Whisper ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            logger.error(traceback.format_exc())
            whisper_model = None
    
    # TTS: ì œê±° (ì‹œê°„ ì ˆì•½, APIë¡œ ëŒ€ì²´ ì˜ˆì •)
    # logger.info("âš ï¸ TTS ë¹„í™œì„±í™” - í…ìŠ¤íŠ¸ ì‘ë‹µë§Œ ì œê³µ")
    
    # LLM: Gemini 1.5 Flash ì´ˆê¸°í™”
    if gemini_model is None:
        try:
            import google.generativeai as genai
            
            api_key = os.getenv("GEMINI_API_KEY")
            if not api_key:
                raise ValueError("GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            genai.configure(api_key=api_key)
            # gemini-2.0-flash (ìµœì‹  ë²„ì „, quota ì œí•œ ì£¼ì˜)
            # gemini-2.0-flash-exp (ì‹¤í—˜ ë²„ì „, quota ì œí•œ ì‹¬í•¨)
            gemini_model = genai.GenerativeModel("gemini-2.0-flash")
            logger.info("âœ… Gemini 2.0 Flash ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ Gemini ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            logger.error(traceback.format_exc())
            gemini_model = None


# ============================================================
# S3ì—ì„œ ìŒì„± íŒŒì¼ ë‹¤ìš´ë¡œë“œ
# ============================================================
def download_audio_from_s3(s3_url: str, user_id: str) -> str:
    """
    S3 URLì—ì„œ ìŒì„± íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    
    Args:
        s3_url: S3 URL (https://bucket.s3.region.amazonaws.com/key)
        user_id: ì‚¬ìš©ì ID (ì„ì‹œ íŒŒì¼ëª…ìš©)
    
    Returns:
        str: ë¡œì»¬ íŒŒì¼ ê²½ë¡œ
    """
    import tempfile
    import urllib.request
    import ssl
    
    # SSL ì»¨í…ìŠ¤íŠ¸ (ì¸ì¦ì„œ ê²€ì¦)
    ssl_context = ssl.create_default_context()
    
    # ì„ì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±
    temp_dir = tempfile.gettempdir()
    local_path = os.path.join(temp_dir, f"audio_{user_id}_{os.getpid()}.m4a")
    
    try:
        logger.info(f"S3 ë‹¤ìš´ë¡œë“œ: {s3_url} -> {local_path}")
        
        # URLë¡œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ (S3 public URL ê°€ì •)
        urllib.request.urlretrieve(s3_url, local_path)
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(local_path):
            raise FileNotFoundError(f"ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì—†ìŒ: {local_path}")
        
        file_size = os.path.getsize(local_path)
        logger.info(f"S3 ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {file_size} bytes")
        
        return local_path
        
    except Exception as e:
        logger.error(f"S3 ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise RuntimeError(f"S3 ìŒì„± íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")


# ============================================================
# Celery íƒœìŠ¤í¬: AI ëª¨ë¸ ì‚¬ì „ ë¡œë“œ
# ============================================================
@celery_app.task(bind=True, name="worker.tasks.preload_models")
def preload_models(self: Task):
    """
    AI ëª¨ë¸ ì‚¬ì „ ë¡œë“œ (Whisper, Qwen3-TTS, Gemini)
    
    ì²« íƒœìŠ¤í¬ ì‹¤í–‰ ì „ì— ì´ taskë¥¼ ì‹¤í–‰í•˜ë©´ ëª¨ë¸ì„ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    Returns:
        dict: ê° ëª¨ë¸ì˜ ë¡œë”© ìƒíƒœ
    """
    try:
        logger.info("ğŸš€ AI ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì‹œì‘...")
        
        load_models()
        
        # ê° ëª¨ë¸ ë¡œë”© ìƒíƒœ í™•ì¸
        status = {
            "whisper": "loaded" if whisper_model is not None else "failed",
            "gemini": "loaded" if gemini_model is not None else "failed",
        }
        
        # ëª¨ë¸ ì €ì¥ ê²½ë¡œ í™•ì¸
        import subprocess
        models_info = subprocess.run(
            ['du', '-sh', f'{settings.models_root}/*'],
            capture_output=True,
            text=True
        )
        
        logger.info("âœ… AI ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì™„ë£Œ!")
        logger.info(f"   ëª¨ë¸ ìƒíƒœ: {status}")
        logger.info(f"   ì €ì¥ ê²½ë¡œ: {settings.models_root}")
        
        return {
            "status": "success",
            "models": status,
            "storage_info": models_info.stdout,
            "message": "ëª¨ë“  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ"
        }
    
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            "status": "error",
            "message": str(e)
        }


# ============================================================
# Celery íƒœìŠ¤í¬: ìŒì„± ëŒ€í™” ì²˜ë¦¬ (STT + Brain + Summary-Buffer Memory)
# ============================================================
@celery_app.task(bind=True, name="worker.tasks.process_audio_and_reply")
def process_audio_and_reply(
    self: Task,
    audio_url: str,
    user_id: str,
    session_id: str = None,
    summary: str = "",
    recent_logs: list = None,
    turn_count: int = 0
):
    """
    ìŒì„± íŒŒì¼ì„ ë°›ì•„ AI ëŒ€í™” ì²˜ë¦¬ (Summary-Buffer Memory ì ìš©)
    
    Flow:
    1. S3ì—ì„œ ìŒì„± íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    2. STT: Faster-Whisperë¡œ ìŒì„± â†’ í…ìŠ¤íŠ¸
    3. Brain: ìš”ì•½ + ìµœê·¼ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¡œ Gemini ì‘ë‹µ ìƒì„±
    4. ë§¤ 3í„´ë§ˆë‹¤ ëŒ€í™” ìš”ì•½ ì—…ë°ì´íŠ¸
    
    Args:
        audio_url: S3 URL (EC2ì—ì„œ ì—…ë¡œë“œë¨)
        user_id: ì‚¬ìš©ì ID
        session_id: ëŒ€í™” ì„¸ì…˜ ID
        summary: í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” ìš”ì•½ (Summary-Buffer Memory)
        recent_logs: ìµœê·¼ ëŒ€í™” ë¡œê·¸ [{"role": "user"|"assistant", "content": "..."}]
        turn_count: í˜„ì¬ ëŒ€í™” í„´ ìˆ˜
    
    Returns:
        dict: {
            "status": "success",
            "user_text": ì‚¬ìš©ì ìŒì„± í…ìŠ¤íŠ¸,
            "ai_reply": AI ë‹µë³€ í…ìŠ¤íŠ¸,
            "sentiment": ê°ì •,
            "new_summary": ì—…ë°ì´íŠ¸ëœ ìš”ì•½ (3í„´ë§ˆë‹¤),
            "session_id": ì„¸ì…˜ ID
        }
    """
    if recent_logs is None:
        recent_logs = []
    
    try:
        # ëª¨ë¸ ë¡œë”© (ì²« ì‹¤í–‰ ì‹œì—ë§Œ)
        load_models()
        
        # S3ì—ì„œ ìŒì„± íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        logger.info(f"[S3] ìŒì„± íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {audio_url}")
        local_audio_path = download_audio_from_s3(audio_url, user_id)
        logger.info(f"[S3] ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local_audio_path}")
        
        # Step 1: STT (ìŒì„± â†’ í…ìŠ¤íŠ¸)
        logger.info(f"[STT] ìŒì„± ì¸ì‹ ì‹œì‘: {local_audio_path}")
        user_text = transcribe_audio(local_audio_path)
        logger.info(f"[STT] ì¸ì‹ ê²°ê³¼: {user_text}")
        
        # ë‹¤ìš´ë¡œë“œí•œ ì„ì‹œ íŒŒì¼ ì‚­ì œ
        try:
            if os.path.exists(local_audio_path):
                os.remove(local_audio_path)
                logger.info(f"[S3] ì„ì‹œ íŒŒì¼ ì‚­ì œ: {local_audio_path}")
        except Exception as e:
            logger.warning(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
        
        # Step 2: Brain (Summary-Buffer Memory ì ìš©í•œ Gemini ì‘ë‹µ ìƒì„±)
        logger.info(f"[Brain] AI ë‹µë³€ ìƒì„± ì¤‘... (í„´ ìˆ˜: {turn_count})")
        reply_data = generate_reply_with_memory(
            user_text=user_text,
            summary=summary,
            recent_logs=recent_logs,
            turn_count=turn_count
        )
        ai_reply = reply_data["text"]
        sentiment = reply_data["sentiment"]
        new_summary = reply_data.get("new_summary")
        
        logger.info(f"[Brain] AI ë‹µë³€: {ai_reply[:50]}... (sentiment={sentiment})")
        if new_summary:
            logger.info(f"[Memory] ìš”ì•½ ì—…ë°ì´íŠ¸: {new_summary[:50]}...")
        
        # AudioChatResult ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë°˜í™˜
        result = {
            "status": "success",
            "user_text": user_text,
            "ai_reply": ai_reply,
            "sentiment": sentiment,
            "session_id": session_id
        }
        
        # ìš”ì•½ì´ ì—…ë°ì´íŠ¸ëœ ê²½ìš°ì—ë§Œ ì¶”ê°€
        if new_summary:
            result["new_summary"] = new_summary
        
        return format_response(
            required_keys=["status", "user_text", "ai_reply", "sentiment", "session_id"],
            data=result,
            schema_name="AudioChatResult"
        )
    
    except Exception as e:
        logger.error(f"âŒ ìŒì„± ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        logger.error(traceback.format_exc())
        return format_response(
            required_keys=["status", "message"],
            data={
                "status": "error",
                "message": str(e)
            },
            schema_name="AudioChatResult"
        )



# ============================================================
# STT: Faster-Whisper
# ============================================================
def transcribe_audio(audio_path: str) -> str:
    """
    ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    
    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
    
    Returns:
        str: ì¸ì‹ëœ í…ìŠ¤íŠ¸
    """
    if whisper_model is None:
        raise RuntimeError("Whisper ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        segments, info = whisper_model.transcribe(
            audio_path,
            language="ko",  # í•œêµ­ì–´
            beam_size=5,
            vad_filter=True  # Voice Activity Detection
        )
        
        # ì„¸ê·¸ë¨¼íŠ¸ ê²°í•©
        text = " ".join([segment.text for segment in segments])
        return text.strip()
    
    except Exception as e:
        logger.error(f"STT ì‹¤íŒ¨: {str(e)}")
        return ""

# ============================================================
# Brain: Summary-Buffer Memory ì ìš© ì‘ë‹µ ìƒì„±
# ============================================================
def generate_reply_with_memory(
    user_text: str,
    summary: str = "",
    recent_logs: list = None,
    turn_count: int = 0
) -> dict:
    """
    Summary-Buffer Memoryë¥¼ ì ìš©í•œ AI ë‹µë³€ ìƒì„±
    
    Args:
        user_text: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
        summary: í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” ìš”ì•½
        recent_logs: ìµœê·¼ ëŒ€í™” ë¡œê·¸ [{"role": "...", "content": "..."}]
        turn_count: í˜„ì¬ ëŒ€í™” í„´ ìˆ˜
    
    Returns:
        dict: {"text": "ë‹µë³€", "sentiment": "...", "new_summary": "..."(ì˜µì…˜)}
    """
    if recent_logs is None:
        recent_logs = []
    
    # Fallback ì‘ë‹µ
    FALLBACK_RESPONSE = {
        "text": "í• ë¨¸ë‹ˆ, ì œê°€ ì˜ ëª» ë“¤ì—ˆì–´ìš”. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”? ë©!",
        "sentiment": "curious"
    }
    
    if gemini_model is None:
        logger.error("Gemini ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return FALLBACK_RESPONSE
    
    try:
        # ìµœê·¼ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        recent_context = ""
        if recent_logs:
            recent_context = "\n".join([
                f"{'ì‚¬ìš©ì' if log['role'] == 'user' else 'AI'}: {log['content']}"
                for log in recent_logs
                if log.get('content')
            ])
        
        # ìš”ì•½ ì—…ë°ì´íŠ¸ í•„ìš” ì—¬ë¶€ (ë§¤ 3í„´ë§ˆë‹¤)
        should_update_summary = (turn_count > 0) and (turn_count % 3 == 0)
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if should_update_summary:
            # ì‘ë‹µ + ìš”ì•½ ì—…ë°ì´íŠ¸ ë™ì‹œ ìš”ì²­ (ê¸°ì¡´ ìš”ì•½ MERGE)
            prompt = f"""
ë‹¹ì‹ ì€ ë…¸ì¸ íšŒìƒ ì¹˜ë£Œë¥¼ ë•ëŠ” ì¹œê·¼í•œ AI ìƒë‹´ì‚¬ 'ë³µì‹¤ì´'ì…ë‹ˆë‹¤.

[ê¸°ì¡´ ëŒ€í™” ìš”ì•½]
{summary if summary else "(ì²« ëŒ€í™”ì…ë‹ˆë‹¤)"}

[ìµœê·¼ ëŒ€í™” ë‚´ìš©]
{recent_context if recent_context else "(ì´ì „ ëŒ€í™” ì—†ìŒ)"}

[í˜„ì¬ ì‚¬ìš©ì ë§]
{user_text}

**AI ë‹µë³€ ì§€ì¹¨:**
1. ë”°ëœ»í•˜ê³  ê³µê°í•˜ëŠ” ì–´ì¡°ë¡œ ëŒ€í™”í•˜ì„¸ìš”.
2. ê³¼ê±° ê¸°ì–µì„ ë– ì˜¬ë¦´ ìˆ˜ ìˆëŠ” ì§ˆë¬¸ì„ í¬í•¨í•˜ì„¸ìš”.
3. 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
4. ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”.
5. ê°€ë” "ë©!" ë˜ëŠ” "ì™ˆì™ˆ!"ì„ ë¶™ì—¬ì£¼ì„¸ìš”.

**ìš”ì•½ ì—…ë°ì´íŠ¸ ì§€ì¹¨ (ì¤‘ìš”!):**
- [ê¸°ì¡´ ëŒ€í™” ìš”ì•½]ì— ìˆëŠ” í•µì‹¬ ì •ë³´(ìŒì‹, ì¥ì†Œ, ì‚¬ëŒ, ì¶”ì–µ ë“±)ë¥¼ ì ˆëŒ€ ì‚­ì œí•˜ì§€ ë§ˆì„¸ìš”.
- [ìµœê·¼ ëŒ€í™” ë‚´ìš©]ê³¼ [í˜„ì¬ ì‚¬ìš©ì ë§]ì—ì„œ ìƒˆë¡­ê²Œ ì•Œê²Œ ëœ ì‚¬ì‹¤ì„ ê¸°ì¡´ ìš”ì•½ì— í†µí•©í•˜ì„¸ìš”.
- 3ì¸ì¹­ ì‹œì ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš” (ì˜ˆ: "ì‚¬ìš©ìëŠ” ...ë¼ê³  ë§í–ˆë‹¤").
- 4-5ë¬¸ì¥ìœ¼ë¡œ ëˆ„ì ëœ ì „ì²´ ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•˜ì„¸ìš”.

**ì¤‘ìš”: ë°˜ë“œì‹œ ì•„ë˜ì˜ JSON í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´ JSONë§Œ!**

{{
    "text": "AI ë‹µë³€ (2-3ë¬¸ì¥)",
    "sentiment": "happy|sad|curious|excited|nostalgic|comforting",
    "new_summary": "[ê¸°ì¡´ ìš”ì•½]ê³¼ [ìƒˆë¡œìš´ ì •ë³´]ë¥¼ í•©ì¹œ ëˆ„ì  ìš”ì•½ (4-5ë¬¸ì¥, ì •ë³´ ëˆ„ë½ ê¸ˆì§€)"
}}
"""
        else:
            # ì¼ë°˜ ì‘ë‹µë§Œ
            prompt = f"""
ë‹¹ì‹ ì€ ë…¸ì¸ íšŒìƒ ì¹˜ë£Œë¥¼ ë•ëŠ” ì¹œê·¼í•œ AI ìƒë‹´ì‚¬ 'ë³µì‹¤ì´'ì…ë‹ˆë‹¤.

[ì´ì „ ëŒ€í™” ìš”ì•½]
{summary if summary else "(ì²« ëŒ€í™”ì…ë‹ˆë‹¤)"}

[ìµœê·¼ ëŒ€í™” ë‚´ìš©]
{recent_context if recent_context else "(ì´ì „ ëŒ€í™” ì—†ìŒ)"}

[í˜„ì¬ ì‚¬ìš©ì ë§]
{user_text}

ì•„ë˜ ì§€ì¹¨ì„ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”:
1. ë”°ëœ»í•˜ê³  ê³µê°í•˜ëŠ” ì–´ì¡°ë¡œ ëŒ€í™”í•˜ì„¸ìš”.
2. ê³¼ê±° ê¸°ì–µì„ ë– ì˜¬ë¦´ ìˆ˜ ìˆëŠ” ì§ˆë¬¸ì„ í¬í•¨í•˜ì„¸ìš”.
3. 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
4. ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”.
5. ê°€ë” "ë©!" ë˜ëŠ” "ì™ˆì™ˆ!"ì„ ë¶™ì—¬ì£¼ì„¸ìš”.

**ì¤‘ìš”: ë°˜ë“œì‹œ ì•„ë˜ì˜ JSON í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´ JSONë§Œ!**

{{
    "text": "AI ë‹µë³€ (2-3ë¬¸ì¥)",
    "sentiment": "happy|sad|curious|excited|nostalgic|comforting"
}}
"""
        
        # Gemini API í˜¸ì¶œ (Retry ë¡œì§)
        import time
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                response = gemini_model.generate_content(prompt)
                break
            except Exception as api_error:
                error_str = str(api_error)
                if "429" in error_str or "quota" in error_str.lower():
                    retry_count += 1
                    import re
                    retry_match = re.search(r'retry in (\d+)', error_str)
                    retry_delay = int(retry_match.group(1)) if retry_match else 10
                    
                    if retry_count < max_retries:
                        logger.warning(f"âš ï¸ Gemini Quota ì´ˆê³¼, {retry_delay}ì´ˆ í›„ ì¬ì‹œë„ ({retry_count}/{max_retries})")
                        time.sleep(retry_delay)
                    else:
                        logger.error("âŒ Gemini Quota ì´ˆê³¼, ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬")
                        return FALLBACK_RESPONSE
                else:
                    raise api_error
        
        # ì‘ë‹µ íŒŒì‹±
        if response and response.text:
            import json
            import re
            
            response_text = response.text.strip()
            
            # ì½”ë“œë¸”ë¡ ì œê±°
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(1)
            
            # { ... } íŒ¨í„´ ì¶”ì¶œ
            json_match = re.search(r'\{[^{}]*\}', response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(0)
            
            try:
                ai_reply = json.loads(response_text)
                
                if "text" in ai_reply and "sentiment" in ai_reply:
                    logger.info(f"[Memory] ë‹µë³€ ì„±ê³µ: {ai_reply['text'][:50]}...")
                    return ai_reply
                else:
                    logger.warning("Gemini ì‘ë‹µì— í•„ìˆ˜ í•„ë“œ ëˆ„ë½")
                    return FALLBACK_RESPONSE
            except json.JSONDecodeError as e:
                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                return {
                    "text": response.text.strip()[:200],
                    "sentiment": "comforting"
                }
        else:
            return FALLBACK_RESPONSE
    
    except Exception as e:
        logger.error(f"Summary-Buffer Memory ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        logger.error(traceback.format_exc())
        return FALLBACK_RESPONSE


# ============================================================
# Brain: Gemini 1.5 Flash (Legacy - ê¸°ì¡´ í˜¸í™˜ìš©)
# ============================================================
def generate_reply(user_text: str, user_id: str, session_id: str = None) -> dict:
    """
    ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ AI ë‹µë³€ ìƒì„± (JSON í˜•ì‹ ë°˜í™˜)
    
    Args:
        user_text: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
        user_id: ì‚¬ìš©ì ID
        session_id: ëŒ€í™” ì„¸ì…˜ ID
    
    Returns:
        dict: {"text": "ë‹µë³€ë‚´ìš©", "sentiment": "happy|sad|curious|comforting"}
    """
    # Safety Fallback ì‘ë‹µ
    FALLBACK_RESPONSE = {
        "text": "í• ë¨¸ë‹ˆ, ì œê°€ ì˜ ëª» ë“¤ì—ˆì–´ìš”. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”? ë©!",
        "sentiment": "curious"
    }
    
    if gemini_model is None:
        logger.error("Gemini ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return FALLBACK_RESPONSE
    
    try:
        # íšŒìƒ ì¹˜ë£Œ í”„ë¡¬í”„íŠ¸ (JSON ê°•ì œ)
        prompt = f"""
ë‹¹ì‹ ì€ ë…¸ì¸ íšŒìƒ ì¹˜ë£Œë¥¼ ë•ëŠ” ì¹œê·¼í•œ AI ìƒë‹´ì‚¬ 'ë³µì‹¤ì´'ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ë§: {user_text}

ì•„ë˜ ì§€ì¹¨ì„ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”:
1. ë”°ëœ»í•˜ê³  ê³µê°í•˜ëŠ” ì–´ì¡°ë¡œ ëŒ€í™”í•˜ì„¸ìš”.
2. ê³¼ê±° ê¸°ì–µì„ ë– ì˜¬ë¦´ ìˆ˜ ìˆëŠ” ì§ˆë¬¸ì„ í¬í•¨í•˜ì„¸ìš”.
3. 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
4. ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”.
5. ê°€ë” "ë©!" ë˜ëŠ” "ì™ˆì™ˆ!"ì„ ë¶™ì—¬ì£¼ì„¸ìš”.

**ì¤‘ìš”: ë°˜ë“œì‹œ ì•„ë˜ì˜ JSON í˜•ì‹ë§Œ, ì½”ë“œë¸”ë¡ ì—†ì´, ë‹¤ë¥¸ í…ìŠ¤íŠ¸(ì„¤ëª…, ì¸ì‚¬, ì•ˆë‚´ ë“±) ì—†ì´, ì˜ˆì‹œ ê·¸ëŒ€ë¡œ, í•œê¸€ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.**
ì˜ˆì‹œ:
{{
    "text": "ë‹µë³€ ë‚´ìš©",
    "sentiment": "happy|sad|curious|excited|nostalgic|comforting"
}}

ì˜¤ì§ ìœ„ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸, ë§ˆí¬ë‹¤ìš´, ì„¤ëª…, ì¸ì‚¬, ì•ˆë‚´, ì½”ë“œë¸”ë¡, ê³µë°± ë“±ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
"""
        
        # Gemini API í˜¸ì¶œ (Retry ë¡œì§ ì¶”ê°€)
        import time
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                response = gemini_model.generate_content(prompt)
                break  # ì„±ê³µ ì‹œ ë£¨í”„ íƒˆì¶œ
            
            except Exception as api_error:
                error_str = str(api_error)
                
                # Quota ì´ˆê³¼ ì—ëŸ¬ ì²˜ë¦¬
                if "429" in error_str or "quota" in error_str.lower():
                    retry_count += 1
                    
                    # Retry delay ì¶”ì¶œ (ì—ëŸ¬ ë©”ì‹œì§€ì—ì„œ)
                    import re
                    retry_match = re.search(r'retry in (\d+)', error_str)
                    retry_delay = int(retry_match.group(1)) if retry_match else 10
                    
                    if retry_count < max_retries:
                        logger.warning(f"âš ï¸ Gemini Quota ì´ˆê³¼, {retry_delay}ì´ˆ í›„ ì¬ì‹œë„ ({retry_count}/{max_retries})")
                        time.sleep(retry_delay)
                    else:
                        logger.error(f"âŒ Gemini Quota ì´ˆê³¼, ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬")
                        return FALLBACK_RESPONSE
                else:
                    # ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ì¦‰ì‹œ Fallback
                    raise api_error
        
        # ì‘ë‹µ íŒŒì‹±
        if response and response.text:
            import json
            import re
            
            # JSON ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì²˜ë¦¬)
            response_text = response.text.strip()
            
            # ```json ... ``` ë˜ëŠ” ``` ... ``` íŒ¨í„´ ì œê±°
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(1)
            
            # { ... } íŒ¨í„´ ì¶”ì¶œ
            json_match = re.search(r'\{[^{}]*\}', response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(0)
            
            try:
                ai_reply = json.loads(response_text)
                
                # í•„ìˆ˜ í•„ë“œ í™•ì¸
                if "text" in ai_reply and "sentiment" in ai_reply:
                    logger.info(f"Gemini ë‹µë³€ ì„±ê³µ: {ai_reply['text'][:50]}...")
                    return ai_reply
                else:
                    logger.warning("Gemini ì‘ë‹µì— í•„ìˆ˜ í•„ë“œ ëˆ„ë½, Fallback ì‚¬ìš©")
                    return FALLBACK_RESPONSE
            except json.JSONDecodeError as e:
                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}, ì›ë³¸: {response_text[:100]}")
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì‚¬ìš©
                return {
                    "text": response.text.strip()[:200],
                    "sentiment": "comforting"
                }
        else:
                logger.warning("Gemini ì‘ë‹µì— í•„ìˆ˜ í•„ë“œ ëˆ„ë½, Fallback ì‚¬ìš©")
                return FALLBACK_RESPONSE
    
    except Exception as e:
        logger.error(f"Gemini ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        logger.error(traceback.format_exc())
        return FALLBACK_RESPONSE


# ============================================================
# ê°ì • ë¶„ì„ (ì• ë‹ˆë©”ì´ì…˜ ì—°ë™ìš©)
# ============================================================
def analyze_sentiment(text: str) -> str:
    """
    AI ë‹µë³€ì˜ ê°ì •ì„ ë¶„ì„í•˜ì—¬ ê°•ì•„ì§€ ì• ë‹ˆë©”ì´ì…˜ ê²°ì •
    
    Args:
        text: AI ë‹µë³€ í…ìŠ¤íŠ¸
    
    Returns:
        str: "happy" | "curious" | "nostalgic" | "excited" | "comforting"
    """
    # í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„
    happy_keywords = ["ì¢‹", "ê¸°ë»", "í–‰ë³µ", "ì›ƒ", "ì¬ë°Œ", "ì‹ ë‚˜", "ë©‹ì§€"]
    curious_keywords = ["ë­", "ì–´ë””", "ëˆ„êµ¬", "ì–¸ì œ", "ì™œ", "ì–´ë–»ê²Œ", "?"]
    nostalgic_keywords = ["ì¶”ì–µ", "ì˜›ë‚ ", "ê·¸ë•Œ", "ê¸°ì–µ", "ì˜ˆì „", "ì–´ë¦´", "ì˜¤ë˜"]
    excited_keywords = ["ì™€", "ìš°ì™€", "ëŒ€ë°•", "ì •ë§", "ì§„ì§œ", "!"]
    
    text_lower = text.lower()
    
    # ìš°ì„ ìˆœìœ„: curious > nostalgic > excited > happy > comforting
    for kw in curious_keywords:
        if kw in text_lower:
            return "curious"
    for kw in nostalgic_keywords:
        if kw in text_lower:
            return "nostalgic"
    for kw in excited_keywords:
        if kw in text_lower:
            return "excited"
    for kw in happy_keywords:
        if kw in text_lower:
            return "happy"
    
    return "comforting"  # ê¸°ë³¸ê°’: ë”°ëœ»í•œ ìœ„ë¡œ


# ============================================================
# Celery íƒœìŠ¤í¬: ì´ë¯¸ì§€ ë¶„ì„ (Gemini Vision)
# ============================================================
@celery_app.task(bind=True, name="worker.tasks.analyze_image")
def analyze_image(self: Task, image_path: str, prompt: str):
    """
    ì´ë¯¸ì§€ë¥¼ Gemini Visionìœ¼ë¡œ ë¶„ì„
    
    Args:
        image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
        prompt: ë¶„ì„ ìš”ì²­ í”„ë¡¬í”„íŠ¸
    
    Returns:
        dict: ë¶„ì„ ê²°ê³¼
    """
    try:
        load_models()
        
        if gemini_model is None:
            raise RuntimeError("Gemini ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì´ë¯¸ì§€ ë¡œë”©
        from PIL import Image
        image = Image.open(image_path)
        
        # Gemini Vision ë¶„ì„
        response = gemini_model.generate_content([prompt, image])
        
        return {
            "status": "success",
            "analysis": response.text.strip()
        }
    
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            "status": "error",
            "message": str(e)
        }


# ============================================================
# Celery íƒœìŠ¤í¬: ì‚¬ì§„ ê¸°ë°˜ ì²« ì¸ì‚¬ ìƒì„± (Gemini Vision)
# ============================================================
@celery_app.task(bind=True, name="worker.tasks.generate_greeting")
def generate_greeting(self: Task, image_url: str, pet_name: str = "ë³µì‹¤ì´", session_id: str = None):
    """
    ì‚¬ì§„ì„ ë¶„ì„í•˜ì—¬ ë§ì¶¤í˜• ì²« ì¸ì‚¬ ìƒì„±
    
    Args:
        image_url: S3 URL ë˜ëŠ” ë¡œì»¬ ì´ë¯¸ì§€ ê²½ë¡œ
        pet_name: ë°˜ë ¤ê²¬ ì´ë¦„ (ê¸°ë³¸: ë³µì‹¤ì´)
        session_id: ì„¸ì…˜ ID
    
    Returns:
        GreetingTaskResult ìŠ¤í‚¤ë§ˆ:
        {
            "status": "success",
            "ai_greeting": "ì¸ì‚¬ ë©”ì‹œì§€",
            "analysis": "ì´ë¯¸ì§€ ë¶„ì„ ìš”ì•½",
            "session_id": "uuid-string"
        }
    """
    # ìŠ¤í‚¤ë§ˆ í•„ìˆ˜ í•„ë“œ
    REQUIRED_KEYS = ["status", "ai_greeting", "session_id"]
    
    # Fallback ì‘ë‹µ
    def fallback_response(message: str = None):
        return format_response(
            required_keys=REQUIRED_KEYS,
            data={
                "status": "success",
                "ai_greeting": message or f"ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” {pet_name}ì˜ˆìš”. ì‚¬ì§„ì´ ì°¸ ì¢‹ì•„ ë³´ì—¬ìš”! ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë– ì„¸ìš”? ë©!",
                "analysis": None,
                "session_id": session_id
            },
            schema_name="GreetingTaskResult"
        )
    
    try:
        load_models()
        
        if gemini_model is None:
            logger.error("Gemini ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return fallback_response()
        
        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (S3 URLì¸ ê²½ìš°)
        local_image_path = None
        image_analysis = None
        
        try:
            if image_url.startswith("http"):
                import tempfile
                import urllib.request
                
                temp_dir = tempfile.gettempdir()
                local_image_path = os.path.join(temp_dir, f"greeting_{session_id or 'temp'}_{os.getpid()}.jpg")
                
                logger.info(f"[Greeting] ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ: {image_url}")
                urllib.request.urlretrieve(image_url, local_image_path)
            else:
                local_image_path = image_url
            
            if not os.path.exists(local_image_path):
                raise FileNotFoundError(f"ì´ë¯¸ì§€ íŒŒì¼ ì—†ìŒ: {local_image_path}")
            
            # ì´ë¯¸ì§€ ë¡œë”©
            from PIL import Image
            image = Image.open(local_image_path)
            logger.info(f"[Greeting] ì´ë¯¸ì§€ ë¡œë”© ì™„ë£Œ: {image.size}")
            
        except Exception as img_error:
            logger.warning(f"[Greeting] ì´ë¯¸ì§€ ë¡œë”© ì‹¤íŒ¨: {img_error}, Fallback ì‚¬ìš©")
            return fallback_response()
        
        # Gemini Visionìœ¼ë¡œ ì‚¬ì§„ ë¶„ì„ ë° ì¸ì‚¬ ìƒì„± (2ë‹¨ê³„)
        # Step 1: ì´ë¯¸ì§€ ë¶„ì„
        analysis_prompt = "ì´ ì‚¬ì§„ì—ì„œ ë³´ì´ëŠ” ì¥ì†Œ, ì¸ë¬¼, ìƒí™©ì„ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”. 50ì ì´ë‚´ë¡œ ë‹µë³€í•˜ì„¸ìš”."
        try:
            analysis_response = gemini_model.generate_content([analysis_prompt, image])
            if analysis_response and analysis_response.text:
                image_analysis = analysis_response.text.strip()[:100]
                logger.info(f"[Greeting] ì´ë¯¸ì§€ ë¶„ì„: {image_analysis}")
        except Exception as e:
            logger.warning(f"[Greeting] ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {e}")
            image_analysis = "ì‚¬ì§„ ë¶„ì„ ì‹¤íŒ¨"
        
        # Step 2: ì¸ì‚¬ ìƒì„±
        greeting_prompt = f"""
ë‹¹ì‹ ì€ ë…¸ì¸ íšŒìƒ ì¹˜ë£Œë¥¼ ë•ëŠ” ì¹œê·¼í•œ AI ë°˜ë ¤ê²¬ '{pet_name}'ì…ë‹ˆë‹¤.
ì‚¬ìš©ìê°€ ë³´ì—¬ì¤€ ì‚¬ì§„ì„ ë³´ê³ , ë”°ëœ»í•˜ê³  ì¹œê·¼í•œ ì²« ì¸ì‚¬ë¥¼ í•´ì£¼ì„¸ìš”.

ê·œì¹™:
1. ì‚¬ì§„ì—ì„œ ë³´ì´ëŠ” ë‚´ìš©(ì¥ì†Œ, ì¸ë¬¼, ìƒí™© ë“±)ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•˜ì„¸ìš”.
2. í˜¸ê¸°ì‹¬ì´ ê°€ë“í•œ ê°•ì•„ì§€ì²˜ëŸ¼ ì§ˆë¬¸ì„ í¬í•¨í•˜ì„¸ìš”.
3. 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë§í•˜ì„¸ìš”.
4. ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ê³ , ê°€ë” "ë©!" ë˜ëŠ” "ì™ˆì™ˆ!"ì„ ë¶™ì—¬ì£¼ì„¸ìš”.
5. í• ë¨¸ë‹ˆ/í• ì•„ë²„ì§€ê°€ ë“£ê¸° ì¢‹ì€ ë”°ëœ»í•œ ì–´ì¡°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

**ì¤‘ìš”: JSON ì—†ì´ ìˆœìˆ˜ ì¸ì‚¬ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.**
"""
        
        try:
            response = gemini_model.generate_content([greeting_prompt, image])
            
            if response and response.text:
                ai_greeting = response.text.strip()
                
                # JSON í˜•ì‹ì´ í¬í•¨ëœ ê²½ìš° í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                import json
                import re
                
                # JSON íŒ¨í„´ ì œê±° ì‹œë„
                json_match = re.search(r'\{[^{}]*"text"\s*:\s*"([^"]+)"', ai_greeting)
                if json_match:
                    ai_greeting = json_match.group(1)
                else:
                    # ì½”ë“œë¸”ë¡ ì œê±°
                    ai_greeting = re.sub(r'```.*?```', '', ai_greeting, flags=re.DOTALL)
                    ai_greeting = ai_greeting.strip()
                
                # 200ì ì œí•œ
                ai_greeting = ai_greeting[:200]
                
                logger.info(f"[Greeting] ìƒì„± ì™„ë£Œ: {ai_greeting[:50]}...")
                
                return format_response(
                    required_keys=REQUIRED_KEYS,
                    data={
                        "status": "success",
                        "ai_greeting": ai_greeting,
                        "analysis": image_analysis,
                        "session_id": session_id
                    },
                    schema_name="GreetingTaskResult"
                )
            
            return fallback_response()
            
        except Exception as api_error:
            logger.error(f"[Greeting] Gemini API ì˜¤ë¥˜: {api_error}")
            return fallback_response()
        
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if local_image_path and local_image_path != image_url:
                try:
                    if os.path.exists(local_image_path):
                        os.remove(local_image_path)
                except:
                    pass
    
    except Exception as e:
        logger.error(f"[Greeting] ì²« ì¸ì‚¬ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        logger.error(traceback.format_exc())
        return format_response(
            required_keys=["status", "message", "session_id"],
            data={
                "status": "failure",
                "message": f"ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}",
                "session_id": session_id
            },
            schema_name="GreetingTaskResult"
        )


# ============================================================
# Celery íƒœìŠ¤í¬: í…ìŠ¤íŠ¸ ì „ìš© ëŒ€í™” (í…ŒìŠ¤íŠ¸ìš©)
# ============================================================
@celery_app.task(bind=True, name="worker.tasks.generate_reply_from_text")
def generate_reply_from_text(self: Task, user_text: str, user_id: str, session_id: str = None):
    """
    í…ìŠ¤íŠ¸ ì…ë ¥ìœ¼ë¡œ AI ë‹µë³€ ìƒì„± + TTS ìŒì„± í•©ì„±
    """
    try:
        load_models()
        
        # Geminië¡œ ë‹µë³€ ìƒì„± (JSON ë°˜í™˜)
        reply_data = generate_reply(user_text, user_id, session_id)
        ai_reply = reply_data["text"]
        sentiment = reply_data["sentiment"]
        logger.info(f"âœ… AI ë‹µë³€ ìƒì„± ì™„ë£Œ: {ai_reply[:50]}... (sentiment={sentiment})")
        
        return {
            "status": "success",
            "user_text": user_text,
            "ai_reply": ai_reply,
            "sentiment": sentiment,
            "session_id": session_id,
            "gemini_response": ai_reply  # í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ í˜¸í™˜
        }
    
    except Exception as e:
        logger.error(f"í…ìŠ¤íŠ¸ ëŒ€í™” ì‹¤íŒ¨: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            "status": "error",
            "message": str(e)
        }


# ============================================================
# Celery íƒœìŠ¤í¬: ê¸°ì–µ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ (Memory Insight Extraction)
# ============================================================
@celery_app.task(bind=True, name="worker.tasks.extract_memory_insights")
def extract_memory_insights(self: Task, session_id: str, chat_logs: list):
    """
    ëŒ€í™” ë‚´ìš©ì—ì„œ ì˜ë¯¸ ìˆëŠ” ê¸°ì–µì„ ì¶”ì¶œ
    
    Args:
        session_id: ëŒ€í™” ì„¸ì…˜ ID (ë¬¸ìì—´)
        chat_logs: ëŒ€í™” ë¡œê·¸ ë¦¬ìŠ¤íŠ¸ [{"role": "user"|"assistant", "content": "..."}]
    
    Returns:
        InsightTaskResult ìŠ¤í‚¤ë§ˆ:
        {
            "status": "success",
            "session_id": "...",
            "insights": [
                {"category": "family", "fact": "ì†ì£¼ì™€ í•¨ê»˜...", "importance": 4},
                ...
            ]
        }
    """
    # ìŠ¤í‚¤ë§ˆ í•„ìˆ˜ í•„ë“œ
    REQUIRED_KEYS = ["status", "session_id", "insights"]
    VALID_CATEGORIES = ["family", "travel", "food", "hobby", "emotion", "other"]
    
    try:
        load_models()
        
        if gemini_model is None:
            logger.error("[Insight] Gemini ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return format_response(
                required_keys=REQUIRED_KEYS,
                data={
                    "status": "failure",
                    "session_id": session_id,
                    "insights": [],
                    "error": "Gemini ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨"
                },
                schema_name="InsightTaskResult"
            )
        
        # ë¹ˆ ëŒ€í™” ë¡œê·¸ ì²´í¬
        if not chat_logs or len(chat_logs) == 0:
            logger.warning(f"[Insight] ì„¸ì…˜ {session_id}ì˜ ëŒ€í™” ë¡œê·¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return format_response(
                required_keys=REQUIRED_KEYS,
                data={
                    "status": "success",
                    "session_id": session_id,
                    "insights": []
                },
                schema_name="InsightTaskResult"
            )
        
        # ëŒ€í™” ë¡œê·¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        conversation_text = "\n".join([
            f"{'ì‚¬ìš©ì' if log['role'] == 'user' else 'AI'}: {log['content']}"
            for log in chat_logs
            if log.get('content')
        ])
        
        logger.info(f"[Insight] ëŒ€í™” ë¶„ì„ ì‹œì‘ (ì„¸ì…˜: {session_id}, ë¡œê·¸ ìˆ˜: {len(chat_logs)})")
        
        # Gemini í”„ë¡¬í”„íŠ¸ (íšŒìƒ ìš”ë²• ì „ë¬¸ê°€ ì—­í• )
        insight_prompt = f"""
ë‹¹ì‹ ì€ ë…¸ì¸ íšŒìƒ ì¹˜ë£Œ(Reminiscence Therapy) ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ëŒ€í™”ì—ì„œ ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ì˜ë¯¸ ìˆëŠ” ê¸°ì–µê³¼ ê°ì •ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

**ê·œì¹™:**
1. ë‹¨ìˆœí•œ ì¸ì‚¬ë‚˜ "ë„¤", "ì•„ë‹ˆì˜¤" ê°™ì€ ë‹µë³€ì€ ë¬´ì‹œí•˜ì„¸ìš”.
2. ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ì¥ì†Œ, ì¸ë¬¼, ìŒì‹, ì·¨ë¯¸, ê°ì • ë“± êµ¬ì²´ì ì¸ ë‚´ìš©ë§Œ ì¶”ì¶œí•˜ì„¸ìš”.
3. ê° ê¸°ì–µì— ëŒ€í•´ ì¤‘ìš”ë„(1-5)ë¥¼ í‰ê°€í•˜ì„¸ìš”:
   - 5: ë§¤ìš° ì¤‘ìš” (ê°€ì¡± ì´ì•¼ê¸°, íŠ¹ë³„í•œ ì¶”ì–µ)
   - 4: ì¤‘ìš” (ì—¬í–‰, ì´ë²¤íŠ¸)
   - 3: ë³´í†µ (ì·¨ë¯¸, ì¼ìƒì  í™œë™)
   - 2: ë‚®ìŒ (ì¼ë°˜ì ì¸ ì„ í˜¸)
   - 1: ë§¤ìš° ë‚®ìŒ (ì‚¬ì†Œí•œ ì–¸ê¸‰)
4. ì¶”ì¶œëœ ì‚¬ì‹¤(fact)ì€ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
5. ì¹´í…Œê³ ë¦¬: family, travel, food, hobby, emotion, other

**ì¤‘ìš”: ë°˜ë“œì‹œ ì•„ë˜ JSON ë°°ì—´ í˜•ì‹ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.**

ì˜ˆì‹œ:
[
    {{"category": "family", "fact": "ì†ì£¼ ë¯¼ìˆ˜ì™€ í•¨ê»˜ ê³µì›ì—ì„œ ë†€ì•˜ë‹¤", "importance": 5}},
    {{"category": "travel", "fact": "ë¶€ì‚° í•´ìš´ëŒ€ì—ì„œ ë°”ë‹¤ë¥¼ ë´¤ë‹¤", "importance": 4}}
]

ì˜ë¯¸ ìˆëŠ” ê¸°ì–µì´ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ []ì„ ë°˜í™˜í•˜ì„¸ìš”.

---
ëŒ€í™” ë‚´ìš©:
{conversation_text}
"""
        
        try:
            response = gemini_model.generate_content(insight_prompt)
            
            if response and response.text:
                import json
                import re
                
                response_text = response.text.strip()
                logger.info(f"[Insight] Gemini ì‘ë‹µ: {response_text[:200]}...")
                
                # JSON ë°°ì—´ ì¶”ì¶œ
                # ì½”ë“œë¸”ë¡ ì œê±°
                json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response_text, re.DOTALL)
                if json_match:
                    response_text = json_match.group(1)
                
                # ë°°ì—´ íŒ¨í„´ ë§¤ì¹­
                array_match = re.search(r'\[.*\]', response_text, re.DOTALL)
                if array_match:
                    response_text = array_match.group(0)
                
                try:
                    insights_raw = json.loads(response_text)
                    
                    # ìœ íš¨ì„± ê²€ì¦ ë° ì •ì œ
                    insights = []
                    for item in insights_raw:
                        if isinstance(item, dict) and "category" in item and "fact" in item:
                            # ì¹´í…Œê³ ë¦¬ ê²€ì¦
                            category = item["category"].lower()
                            if category not in VALID_CATEGORIES:
                                category = "other"
                            
                            # ì¤‘ìš”ë„ ê²€ì¦ (1-5 ë²”ìœ„)
                            importance = int(item.get("importance", 3))
                            importance = max(1, min(5, importance))
                            
                            insights.append({
                                "category": category,
                                "fact": str(item["fact"]),
                                "importance": importance
                            })
                    
                    logger.info(f"[Insight] ì¶”ì¶œ ì™„ë£Œ: {len(insights)}ê°œ ì¸ì‚¬ì´íŠ¸")
                    
                    return format_response(
                        required_keys=REQUIRED_KEYS,
                        data={
                            "status": "success",
                            "session_id": session_id,
                            "insights": insights
                        },
                        schema_name="InsightTaskResult"
                    )
                    
                except json.JSONDecodeError as e:
                    logger.warning(f"[Insight] JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                    return format_response(
                        required_keys=REQUIRED_KEYS,
                        data={
                            "status": "failure",
                            "session_id": session_id,
                            "insights": [],
                            "error": f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}"
                        },
                        schema_name="InsightTaskResult"
                    )
            
            # ì‘ë‹µì´ ì—†ëŠ” ê²½ìš°
            return format_response(
                required_keys=REQUIRED_KEYS,
                data={
                    "status": "success",
                    "session_id": session_id,
                    "insights": []
                },
                schema_name="InsightTaskResult"
            )
            
        except Exception as api_error:
            logger.error(f"[Insight] Gemini API ì˜¤ë¥˜: {api_error}")
            return format_response(
                required_keys=REQUIRED_KEYS,
                data={
                    "status": "failure",
                    "session_id": session_id,
                    "insights": [],
                    "error": str(api_error)
                },
                schema_name="InsightTaskResult"
            )
    
    except Exception as e:
        logger.error(f"[Insight] ê¸°ì–µ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        logger.error(traceback.format_exc())
        return format_response(
            required_keys=["status", "session_id", "error"],
            data={
                "status": "failure",
                "session_id": session_id,
                "insights": [],
                "error": str(e)
            },
            schema_name="InsightTaskResult"
        )


# ============================================================
# Celery íƒœìŠ¤í¬: ì¶”ì–µ ì˜ìƒ ìƒì„±
# ============================================================
@celery_app.task(bind=True, name="worker.tasks.generate_memory_video")
def generate_memory_video(
    self: Task,
    session_id: str,
    video_id: str,
    video_type: str = "slideshow"
):
    """
    ëŒ€í™” ì„¸ì…˜ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì–µ ì˜ìƒ ìƒì„±

    Flow:
    1. SessionPhotoì—ì„œ ì‚¬ì§„ ëª©ë¡ ì¡°íšŒ
    2. ë‚´ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± (Gemini)
    3. TTS ë‚´ë ˆì´ì…˜ ìƒì„± (Qwen3-TTS)
    4. ì˜ìƒ ìƒì„± (slideshow: FFmpeg / ai_animated: Replicate SVD)
    5. S3 ì—…ë¡œë“œ

    Args:
        session_id: ëŒ€í™” ì„¸ì…˜ ID
        video_id: ìƒì„±í•  ì˜ìƒ ID
        video_type: "slideshow" (FFmpeg) ë˜ëŠ” "ai_animated" (Replicate SVD)

    Returns:
        dict: ì˜ìƒ URL ë° ìƒíƒœ
    """
    db = None
    video = None
    temp_files = []  # ì •ë¦¬í•  ì„ì‹œ íŒŒì¼ë“¤

    try:
        from common.database import SessionLocal
        from common.models import (
            ChatSession, GeneratedVideo, ChatLog, VideoStatus,
            SessionPhoto, VideoType
        )
        from worker.ffmpeg_client import generate_slideshow, get_video_duration
        from common.s3_client import upload_video, download_image

        db = SessionLocal()

        # ì„¸ì…˜ ì¡°íšŒ
        session = db.query(ChatSession).filter(ChatSession.id == session_id).first()
        if not session:
            raise ValueError(f"ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {session_id}")

        # ì˜ìƒ ë ˆì½”ë“œ ì¡°íšŒ
        video = db.query(GeneratedVideo).filter(GeneratedVideo.id == video_id).first()
        if not video:
            raise ValueError(f"ì˜ìƒ ë ˆì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_id}")

        # ìƒíƒœ ì—…ë°ì´íŠ¸: PROCESSING
        video.status = VideoStatus.PROCESSING
        db.commit()

        # ============================================================
        # Step 1: ì‚¬ì§„ ìˆ˜ì§‘
        # ============================================================
        logger.info(f"[ì˜ìƒ ìƒì„±] Step 1: ì‚¬ì§„ ìˆ˜ì§‘ (session_id={session_id})")

        # SessionPhoto í…Œì´ë¸”ì—ì„œ ìˆœì„œëŒ€ë¡œ ì¡°íšŒ
        session_photos = (
            db.query(SessionPhoto)
            .filter(SessionPhoto.session_id == session_id)
            .order_by(SessionPhoto.display_order)
            .all()
        )

        # SessionPhotoê°€ ì—†ìœ¼ë©´ main_photoë¡œ fallback
        if not session_photos:
            if not session.main_photo:
                raise ValueError("ì„¸ì…˜ì— ì‚¬ì§„ì´ ì—†ìŠµë‹ˆë‹¤.")
            # main_photoëŠ” UserPhoto ê°ì²´ì´ë¯€ë¡œ s3_url ì†ì„±ì´ ìˆìŒ
            # í•˜ì§€ë§Œ ì•ˆì „ì„ ìœ„í•´ dummy ê°ì²´ë„ s3_urlë¡œ í†µì¼
            photo_records = [type('obj', (object,), {'s3_url': session.main_photo.s3_url if hasattr(session.main_photo, 's3_url') else str(session.main_photo)})]
        else:
            photo_records = session_photos

        # ì‚¬ì§„ ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬
        local_photo_paths = []
        for i, photo in enumerate(photo_records):
            try:
                # S3ì—ì„œ ë‹¤ìš´ë¡œë“œ
                photo_url = photo.s3_url
                local_path = f"/tmp/photo_{video_id}_{i}.jpg"
                temp_files.append(local_path)
                
                download_image(photo_url, local_path)
                
                # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ë¦¬ì‚¬ì´ì¦ˆ, í¬ë§· í†µì¼)
                processed_path = f"/tmp/photo_{video_id}_{i}_processed.jpg"
                temp_files.append(processed_path)

                # íŒŒì¼ ê¸°ë°˜ ì „ì²˜ë¦¬ í•¨ìˆ˜ ì‚¬ìš© (preprocess_image_file)
                preprocess_image_file(
                    local_path,
                    processed_path,
                    target_size=(1920, 1080),  # Full HD
                    jpeg_quality=95
                )
                
                local_photo_paths.append(processed_path)
                logger.info(f"   ì‚¬ì§„ {i+1}/{len(photo_records)} ì¤€ë¹„ ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"   ì‚¬ì§„ {i+1} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                continue

        if not local_photo_paths:
            raise ValueError("ì²˜ë¦¬ ê°€ëŠ¥í•œ ì‚¬ì§„ì´ ì—†ìŠµë‹ˆë‹¤.")

        logger.info(f"[ì˜ìƒ ìƒì„±] {len(local_photo_paths)}ì¥ ì‚¬ì§„ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")

        # ============================================================
        # Step 2: ëŒ€í™” ë¡œê·¸ ìˆ˜ì§‘ ë° ë‚´ë ˆì´ì…˜ ìƒì„±
        # ============================================================
        logger.info(f"[ì˜ìƒ ìƒì„±] Step 2: ë‚´ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±")

        logs = db.query(ChatLog).filter(ChatLog.session_id == session_id).all()
        conversation_text = "\n".join([
            f"{'ì‚¬ìš©ì' if log.role == 'user' else 'ê°•ì•„ì§€'}: {log.content}"
            for log in logs
        ])

        if gemini_model is None:
            load_models()

        photo_count = len(local_photo_paths)
        narration_prompt = f"""ë‹¤ìŒì€ í• ë¨¸ë‹ˆì™€ ë°˜ë ¤ê²¬ AIì˜ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤.

{conversation_text}

ì´ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì†ì£¼ê°€ í• ë¨¸ë‹ˆì—ê²Œ ë“¤ë ¤ì£¼ëŠ” ë”°ëœ»í•œ ë‚´ë ˆì´ì…˜**ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
{photo_count}ì¥ì˜ ì‚¬ì§„ì´ ìŠ¬ë¼ì´ë“œì‡¼ë¡œ ë³´ì—¬ì§ˆ ì˜ˆì •ì…ë‹ˆë‹¤.
ì „ì²´ 3-5ë¬¸ì¥ìœ¼ë¡œ ë”°ëœ»í•˜ê³  ê°ë™ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë‚´ë ˆì´ì…˜:"""

        response = gemini_model.generate_content(narration_prompt)
        narration_text = response.text.strip()
        logger.info(f"[ì˜ìƒ ìƒì„±] ë‚´ë ˆì´ì…˜: {narration_text[:100]}...")

        # ============================================================
        # Step 3: ë°°ê²½ìŒì•… ì„¤ì • (TTS ì œê±°ë¨)
        # ============================================================
        logger.info(f"[ì˜ìƒ ìƒì„±] Step 3: ë°°ê²½ìŒì•… ì„¤ì • (TTS ì œê±°)")
        # BGM íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¬´ìŒìœ¼ë¡œ ì²˜ë¦¬
        bgm_path = os.path.join(settings.models_root, "bgm", "emotional_bgm.mp3")
        if os.path.exists(bgm_path):
            narration_audio_path = bgm_path
            logger.info(f"[ì˜ìƒ ìƒì„±] BGM ì‚¬ìš©: {bgm_path}")
        else:
            narration_audio_path = None
            logger.warning(f"[ì˜ìƒ ìƒì„±] BGM íŒŒì¼ ì—†ìŒ, ë¬´ìŒìœ¼ë¡œ ìƒì„±")

        # ============================================================
        # Step 4: ì˜ìƒ ìƒì„±
        # ============================================================
        logger.info(f"[ì˜ìƒ ìƒì„±] Step 4: ì˜ìƒ ë Œë”ë§ (type={video_type})")
        output_video_path = f"/tmp/video_{video_id}.mp4"
        temp_files.append(output_video_path)

        if video_type == "slideshow":
            # FFmpeg ìŠ¬ë¼ì´ë“œì‡¼ ìƒì„±
            generate_slideshow(
                image_paths=local_photo_paths,
                audio_path=narration_audio_path,  # BGM ë˜ëŠ” None
                output_path=output_video_path,
                duration_per_slide=4.0  # ì‚¬ì§„ë‹¹ 4ì´ˆ (ì–´ë¥´ì‹  ì‹œì²­ ê³ ë ¤)
            )
        else:
            # Replicate SVD ì• ë‹ˆë©”ì´ì…˜ (ê¸°ì¡´ ë¡œì§)
            from common.replicate_client import generate_animated_video
            
            svd_outputs = []
            for img_path in local_photo_paths:
                video_url = generate_animated_video(img_path)
                svd_outputs.append(video_url)
            
            # ì˜ìƒ ë³‘í•© + ì˜¤ë””ì˜¤ ì¶”ê°€
            from worker.ffmpeg_client import merge_videos_with_audio
            merge_videos_with_audio(
                video_paths=svd_outputs,
                audio_path=narration_audio_path,
                output_path=output_video_path
            )

        # ============================================================
        # Step 5: S3 ì—…ë¡œë“œ
        # ============================================================
        logger.info(f"[ì˜ìƒ ìƒì„±] Step 5: S3 ì—…ë¡œë“œ")

        video_url, thumbnail_url = upload_video(
            output_video_path,
            str(session.user_id),
            str(video_id)
        )

        # ì˜ìƒ ê¸¸ì´ ì¡°íšŒ
        duration = get_video_duration(output_video_path)

        # ============================================================
        # Step 6: DB ì—…ë°ì´íŠ¸
        # ============================================================
        video.video_url = video_url
        video.thumbnail_url = thumbnail_url
        video.status = VideoStatus.COMPLETED
        video.video_type = VideoType(video_type)
        video.duration_seconds = duration
        db.commit()

        logger.info(f"[ì˜ìƒ ìƒì„±] âœ… ì™„ë£Œ: {video_url} ({duration:.1f}ì´ˆ)")

        return {
            "status": "success",
            "video_id": str(video_id),
            "video_url": video_url,
            "thumbnail_url": thumbnail_url,
            "duration_seconds": duration
        }

    except Exception as e:
        logger.error(f"[ì˜ìƒ ìƒì„±] âŒ ì‹¤íŒ¨: {str(e)}")
        logger.error(traceback.format_exc())

        # ìƒíƒœ ì—…ë°ì´íŠ¸: FAILED
        if db and video:
            try:
                video.status = VideoStatus.FAILED
                db.commit()
            except:
                pass

        return {
            "status": "error",
            "message": str(e)
        }

    finally:
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        for temp_file in temp_files:
            try:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
            except:
                pass
        
        if db:
            db.close()


# ============================================================
# Celery íƒœìŠ¤í¬: ê¸°ì–µ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
# ============================================================
@celery_app.task(bind=True, name="worker.tasks.extract_memory_insights")
def extract_memory_insights(self: Task, session_id: str):
    """
    ëŒ€í™” ì¢…ë£Œ ì‹œ ê¸°ì–µ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ (ë°±ê·¸ë¼ìš´ë“œ)
    
    Args:
        session_id: ëŒ€í™” ì„¸ì…˜ ID
    """
    from common.database import get_db_session
    from common.models import ChatSession, ChatLog, MemoryInsight
    
    db = None
    try:
        db = get_db_session()
        
        # ì„¸ì…˜ ì¡°íšŒ
        session = db.query(ChatSession).filter(ChatSession.id == session_id).first()
        if not session:
            logger.warning(f"ì„¸ì…˜ ì—†ìŒ: {session_id}")
            return
        
        # ëª¨ë“  ëŒ€í™” ë¡œê·¸ ì¡°íšŒ
        logs = db.query(ChatLog).filter(ChatLog.session_id == session_id).order_by(ChatLog.created_at).all()
        
        if not logs:
            logger.info(f"ëŒ€í™” ë¡œê·¸ ì—†ìŒ: {session_id}")
            return
        
        # ëŒ€í™” ë‚´ìš© í•©ì¹˜ê¸°
        conversation_text = "\n".join([f"{log.role}: {log.content}" for log in logs])
        
        # Geminië¡œ ê¸°ì–µ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
        prompt = f"""
ë‹¤ìŒì€ ë…¸ì¸ê³¼ AI ê°•ì•„ì§€ì˜ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤. ì´ ëŒ€í™”ì—ì„œ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” ì¤‘ìš”í•œ ê¸°ì–µ ì •ë³´ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.

ëŒ€í™” ë‚´ìš©:
{conversation_text}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ JSONìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
  "insights": [
    {{
      "category": "family|travel|food|hobby|etc",
      "fact": "ì¶”ì¶œëœ ì‚¬ì‹¤",
      "importance": 1-5
    }}
  ]
}}

ì£¼ì˜:
- categoryëŠ” family, travel, food, hobby ì¤‘ í•˜ë‚˜
- factëŠ” êµ¬ì²´ì ì¸ ì‚¬ì‹¤ë§Œ
- importanceëŠ” 1(ë‚®ìŒ)ì—ì„œ 5(ë†’ìŒ)
"""
        
        # ëª¨ë¸ ë¡œë”©
        load_models()
        if gemini_model is None:
            logger.error("Gemini ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
            return
        
        response = gemini_model.generate_content(prompt)
        
        if response and response.text:
            import json
            import re
            
            # JSON íŒŒì‹±
            response_text = response.text.strip()
            json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(1)
            
            try:
                data = json.loads(response_text)
                insights = data.get("insights", [])
                
                # DB ì €ì¥
                for insight in insights:
                    memory_insight = MemoryInsight(
                        user_id=session.user_id,
                        category=insight["category"],
                        fact=insight["fact"],
                        importance=insight["importance"],
                        source_log_id=logs[-1].id if logs else None  # ë§ˆì§€ë§‰ ë¡œê·¸
                    )
                    db.add(memory_insight)
                
                db.commit()
                logger.info(f"ê¸°ì–µ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(insights)}ê°œ")
                
            except json.JSONDecodeError as e:
                logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        
    except Exception as e:
        logger.error(f"ê¸°ì–µ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        logger.error(traceback.format_exc())
    
    finally:
        if db:
            db.close()