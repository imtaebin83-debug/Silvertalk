"""
Celery AI ì²˜ë¦¬ íƒœìŠ¤í¬ (TTS ì œê±° ë²„ì „)
- ë™ì  ë””ë°”ì´ìŠ¤ ê°ì§€ (CUDA/CPU ìë™ ì „í™˜)
- AI ëª¨ë¸: Faster-Whisper (STT), Gemini (LLM)
- TTSëŠ” í´ë¼ì´ì–¸íŠ¸ expo-speechë¡œ ëŒ€ì²´
- CUDA ì¶©ëŒ ë°©ì§€: Lazy ì´ˆê¸°í™” + ëª…ì‹œì  .env ë¡œë“œ
"""

# ============================================================
# ìµœìš°ì„ : .env ë¡œë“œ (GEMINI_API_KEY ë“±)
# ============================================================
from dotenv import load_dotenv
load_dotenv()

import os
import logging
import traceback
import soundfile as sf
from celery import Task
from worker.celery_app import celery_app
from common.config import settings
from common.image_utils import preprocess_image_for_ai, ImageProcessingError

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================
# ë™ì  ë””ë°”ì´ìŠ¤ ê°ì§€ (Lazy ì´ˆê¸°í™”)
# ============================================================
_device_cache = None
DEVICE = None
COMPUTE_TYPE = None

def detect_device():
    """
    ì‹¤í–‰ í™˜ê²½ì— ë”°ë¼ ìë™ìœ¼ë¡œ ë””ë°”ì´ìŠ¤ ì„ íƒ
    Worker fork ì´í›„ì— í˜¸ì¶œë˜ì–´ì•¼ CUDA ì¶©ëŒ ë°©ì§€
    
    Returns:
        tuple: (device, compute_type)
    """
    global _device_cache, DEVICE, COMPUTE_TYPE
    
    if _device_cache is not None:
        return _device_cache
    
    # torchëŠ” í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ import (Worker fork ì´í›„)
    try:
        import torch
        
        if torch.cuda.is_available():
            device = "cuda"
            compute_type = "float16"  # Whisperìš©
            gpu_name = torch.cuda.get_device_name(0)
            logger.info(f"ğŸš€ GPU ê°ì§€: {gpu_name} - CUDA ëª¨ë“œ í™œì„±í™”")
        else:
            device = "cpu"
            compute_type = "int8"
            logger.info("ğŸ’» GPU ë¯¸ê°ì§€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        
        _device_cache = (device, compute_type)
        DEVICE = device
        COMPUTE_TYPE = compute_type
        return _device_cache
    
    except Exception as e:
        logger.error(f"âŒ ë””ë°”ì´ìŠ¤ ê°ì§€ ì‹¤íŒ¨: {str(e)}, CPUë¡œ fallback")
        _device_cache = ("cpu", "int8")
        DEVICE = "cpu"
        COMPUTE_TYPE = "int8"
        return _device_cache


# ============================================================
# AI ëª¨ë¸ ì „ì—­ ë³€ìˆ˜ (ì›Œì»¤ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ë¡œë“œ)
# ============================================================
whisper_model = None
# tts_model = None  # TTS ì œê±° (ì‹œê°„ ì ˆì•½)
gemini_model = None


# ============================================================
# ëª¨ë¸ ë¡œë”© í•¨ìˆ˜
# ============================================================
def load_models():
    """
    AI ëª¨ë¸ ì´ˆê¸°í™” (ì›Œì»¤ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰)
    
    ì—ëŸ¬ ì²˜ë¦¬:
    - ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì‹œì—ë„ ì›Œì»¤ê°€ ì£½ì§€ ì•Šë„ë¡ try-except ì‚¬ìš©
    """
    global whisper_model, tts_model, gemini_model
    
    # ë””ë°”ì´ìŠ¤ ê°ì§€ (ì²« ì‹¤í–‰)
    device, compute_type = detect_device()
    logger.info(f"ğŸ”§ ë””ë°”ì´ìŠ¤ ì„¤ì •: device={device}, compute_type={compute_type}")
    
    # STT: Faster-Whisper ë¡œë”©
    if whisper_model is None:
        try:
            from faster_whisper import WhisperModel
            
            # RunPod Volume ê²½ë¡œ ì‚¬ìš© (ì˜êµ¬ ì €ì¥)
            whisper_root = os.path.join(settings.models_root, "whisper")
            os.makedirs(whisper_root, exist_ok=True)
            
            logger.info(f"[Whisper] ëª¨ë¸ ë¡œë”© ì‹œì‘... (ê²½ë¡œ: {whisper_root})")
            
            # CUDA ì‹œë„ (cuDNN + libcublas ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ì „ ì²´í¬)
            if device == "cuda":
                try:
                    # CUDA í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ì „ ì²´í¬
                    import ctypes
                    
                    # 1. cuDNN ì²´í¬
                    try:
                        ctypes.CDLL("libcudnn_ops_infer.so.8")
                        logger.info("âœ… cuDNN ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸ ì™„ë£Œ")
                    except OSError as e:
                        raise Exception(f"cuDNN ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½: {e}")
                    
                    # 2. libcublas ì²´í¬ (CUDA 11.x ë˜ëŠ” 12.x)
                    cublas_found = False
                    for cublas_ver in ["libcublas.so.11", "libcublas.so.12"]:
                        try:
                            ctypes.CDLL(cublas_ver)
                            logger.info(f"âœ… {cublas_ver} í™•ì¸ ì™„ë£Œ")
                            cublas_found = True
                            break
                        except OSError:
                            continue
                    
                    if not cublas_found:
                        raise Exception("libcublas ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½ (11 ë˜ëŠ” 12 ë²„ì „ í•„ìš”)")
                    
                    # ë¼ì´ë¸ŒëŸ¬ë¦¬ ì²´í¬ í†µê³¼ â†’ Whisper ëª¨ë¸ ë¡œë”©
                    whisper_model = WhisperModel(
                        model_size_or_path="medium",
                        device=device,
                        compute_type=compute_type,
                        download_root=whisper_root
                    )
                    logger.info(f"âœ… Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ (model=medium, device={device}, path={whisper_root})")
                
                except Exception as cuda_error:
                    logger.warning(f"âš ï¸ CUDA ë¼ì´ë¸ŒëŸ¬ë¦¬ ì²´í¬ ì‹¤íŒ¨: {cuda_error}")
                    logger.warning("âš ï¸ CPU ëª¨ë“œë¡œ ê°•ì œ ì „í™˜")
                    whisper_model = WhisperModel(
                        model_size_or_path="medium",
                        device="cpu",
                        compute_type="int8",
                        download_root=whisper_root
                    )
                    logger.info(f"âœ… Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ (model=medium, device=cpu, path={whisper_root})")
            
            else:
                # CPU ëª¨ë“œ ì§ì ‘ ë¡œë”©
                whisper_model = WhisperModel(
                    model_size_or_path="medium",
                    device="cpu",
                    compute_type="int8",
                    download_root=whisper_root
                )
                logger.info(f"âœ… Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ (model=medium, device=cpu, path={whisper_root})")
        except Exception as e:
            logger.error(f"âŒ Whisper ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            logger.error(traceback.format_exc())
            whisper_model = None
    
    # TTS: ì œê±° (ì‹œê°„ ì ˆì•½, APIë¡œ ëŒ€ì²´ ì˜ˆì •)
    # logger.info("âš ï¸ TTS ë¹„í™œì„±í™” - í…ìŠ¤íŠ¸ ì‘ë‹µë§Œ ì œê³µ")
    
    # LLM: Gemini 1.5 Flash ì´ˆê¸°í™”
    if gemini_model is None:
        try:
            import google.generativeai as genai
            
            api_key = os.getenv("GEMINI_API_KEY")
            if not api_key:
                raise ValueError("GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            genai.configure(api_key=api_key)
            # gemini-2.0-flash-exp (ìµœì‹  ë²„ì „, íŒ€ì› ê²€ì¦ ì™„ë£Œ)
            gemini_model = genai.GenerativeModel("gemini-2.0-flash-exp")
            logger.info("âœ… Gemini 2.0 Flash ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ Gemini ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            logger.error(traceback.format_exc())
            gemini_model = None


# ============================================================
# S3ì—ì„œ ìŒì„± íŒŒì¼ ë‹¤ìš´ë¡œë“œ
# ============================================================
def download_audio_from_s3(s3_url: str, user_id: str) -> str:
    """
    S3 URLì—ì„œ ìŒì„± íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    
    Args:
        s3_url: S3 URL (https://bucket.s3.region.amazonaws.com/key)
        user_id: ì‚¬ìš©ì ID (ì„ì‹œ íŒŒì¼ëª…ìš©)
    
    Returns:
        str: ë¡œì»¬ íŒŒì¼ ê²½ë¡œ
    """
    import tempfile
    import urllib.request
    import ssl
    
    # SSL ì»¨í…ìŠ¤íŠ¸ (ì¸ì¦ì„œ ê²€ì¦)
    ssl_context = ssl.create_default_context()
    
    # ì„ì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±
    temp_dir = tempfile.gettempdir()
    local_path = os.path.join(temp_dir, f"audio_{user_id}_{os.getpid()}.m4a")
    
    try:
        logger.info(f"S3 ë‹¤ìš´ë¡œë“œ: {s3_url} -> {local_path}")
        
        # URLë¡œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ (S3 public URL ê°€ì •)
        urllib.request.urlretrieve(s3_url, local_path)
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(local_path):
            raise FileNotFoundError(f"ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì—†ìŒ: {local_path}")
        
        file_size = os.path.getsize(local_path)
        logger.info(f"S3 ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {file_size} bytes")
        
        return local_path
        
    except Exception as e:
        logger.error(f"S3 ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        raise RuntimeError(f"S3 ìŒì„± íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")


# ============================================================
# Celery íƒœìŠ¤í¬: AI ëª¨ë¸ ì‚¬ì „ ë¡œë“œ
# ============================================================
@celery_app.task(bind=True, name="worker.tasks.preload_models")
def preload_models(self: Task):
    """
    AI ëª¨ë¸ ì‚¬ì „ ë¡œë“œ (Whisper, Qwen3-TTS, Gemini)
    
    ì²« íƒœìŠ¤í¬ ì‹¤í–‰ ì „ì— ì´ taskë¥¼ ì‹¤í–‰í•˜ë©´ ëª¨ë¸ì„ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    Returns:
        dict: ê° ëª¨ë¸ì˜ ë¡œë”© ìƒíƒœ
    """
    try:
        logger.info("ğŸš€ AI ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì‹œì‘...")
        
        load_models()
        
        # ê° ëª¨ë¸ ë¡œë”© ìƒíƒœ í™•ì¸
        status = {
            "whisper": "loaded" if whisper_model is not None else "failed",
            "gemini": "loaded" if gemini_model is not None else "failed",
        }
        
        # ëª¨ë¸ ì €ì¥ ê²½ë¡œ í™•ì¸
        import subprocess
        models_info = subprocess.run(
            ['du', '-sh', f'{settings.models_root}/*'],
            capture_output=True,
            text=True
        )
        
        logger.info("âœ… AI ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì™„ë£Œ!")
        logger.info(f"   ëª¨ë¸ ìƒíƒœ: {status}")
        logger.info(f"   ì €ì¥ ê²½ë¡œ: {settings.models_root}")
        
        return {
            "status": "success",
            "models": status,
            "storage_info": models_info.stdout,
            "message": "ëª¨ë“  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ"
        }
    
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            "status": "error",
            "message": str(e)
        }


# ============================================================
# Celery íƒœìŠ¤í¬: ìŒì„± ëŒ€í™” ì²˜ë¦¬ (STT + Brain + TTS)
# ============================================================
@celery_app.task(bind=True, name="worker.tasks.process_audio_and_reply")
def process_audio_and_reply(self: Task, audio_url: str, user_id: str, session_id: str = None):
    """
    ìŒì„± íŒŒì¼ì„ ë°›ì•„ AI ëŒ€í™” ì²˜ë¦¬
    
    Flow:
    1. S3ì—ì„œ ìŒì„± íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    2. STT: Faster-Whisperë¡œ ìŒì„± â†’ í…ìŠ¤íŠ¸
    3. Brain: Geminië¡œ ëŒ€í™” ìƒì„±
    
    Args:
        audio_url: S3 URL (EC2ì—ì„œ ì—…ë¡œë“œë¨)
        user_id: ì‚¬ìš©ì ID
        session_id: ëŒ€í™” ì„¸ì…˜ ID (ì˜µì…˜)
    
    Returns:
        dict: {
            "user_text": ì‚¬ìš©ì ìŒì„± í…ìŠ¤íŠ¸,
            "ai_reply": AI ë‹µë³€ í…ìŠ¤íŠ¸,
            "sentiment": ê°ì •
        }
    """
    try:
        # ëª¨ë¸ ë¡œë”© (ì²« ì‹¤í–‰ ì‹œì—ë§Œ)
        load_models()
        
        # S3ì—ì„œ ìŒì„± íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        logger.info(f"[S3] ìŒì„± íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {audio_url}")
        local_audio_path = download_audio_from_s3(audio_url, user_id)
        logger.info(f"[S3] ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local_audio_path}")
        
        # Step 1: STT (ìŒì„± â†’ í…ìŠ¤íŠ¸)
        logger.info(f"[STT] ìŒì„± ì¸ì‹ ì‹œì‘: {local_audio_path}")
        user_text = transcribe_audio(local_audio_path)
        logger.info(f"[STT] ì¸ì‹ ê²°ê³¼: {user_text}")
        
        # ë‹¤ìš´ë¡œë“œí•œ ì„ì‹œ íŒŒì¼ ì‚­ì œ
        try:
            if os.path.exists(local_audio_path):
                os.remove(local_audio_path)
                logger.info(f"[S3] ì„ì‹œ íŒŒì¼ ì‚­ì œ: {local_audio_path}")
        except Exception as e:
            logger.warning(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
        
        # Step 2: Brain (Geminië¡œ ëŒ€í™” ìƒì„± - JSON ë°˜í™˜)
        logger.info(f"[Brain] AI ë‹µë³€ ìƒì„± ì¤‘...")
        reply_data = generate_reply(user_text, user_id, session_id)
        ai_reply = reply_data["text"]
        sentiment = reply_data["sentiment"]
        logger.info(f"[Brain] AI ë‹µë³€: {ai_reply} (sentiment={sentiment})")
        
        logger.info(f"[ì™„ë£Œ] í…ìŠ¤íŠ¸ ì‘ë‹µ ìƒì„± ì™„ë£Œ (sentiment={sentiment})")
        
        return {
            "status": "success",
            "user_text": user_text,
            "ai_reply": ai_reply,
            "sentiment": sentiment,
            "session_id": session_id  # í´ë¼ì´ì–¸íŠ¸ì—ì„œ DB ì €ì¥ìš©
        }
    
    except Exception as e:
        logger.error(f"âŒ ìŒì„± ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            "status": "error",
            "message": str(e)
        }


# ============================================================
# STT: Faster-Whisper
# ============================================================
def transcribe_audio(audio_path: str) -> str:
    """
    ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    
    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
    
    Returns:
        str: ì¸ì‹ëœ í…ìŠ¤íŠ¸
    """
    if whisper_model is None:
        raise RuntimeError("Whisper ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        segments, info = whisper_model.transcribe(
            audio_path,
            language="ko",  # í•œêµ­ì–´
            beam_size=5,
            vad_filter=True  # Voice Activity Detection
        )
        
        # ì„¸ê·¸ë¨¼íŠ¸ ê²°í•©
        text = " ".join([segment.text for segment in segments])
        return text.strip()
    
    except Exception as e:
        logger.error(f"STT ì‹¤íŒ¨: {str(e)}")
        return ""


# ============================================================
# Brain: Gemini 1.5 Flash
# ============================================================
def generate_reply(user_text: str, user_id: str, session_id: str = None) -> dict:
    """
    ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ AI ë‹µë³€ ìƒì„± (JSON í˜•ì‹ ë°˜í™˜)
    
    Args:
        user_text: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
        user_id: ì‚¬ìš©ì ID
        session_id: ëŒ€í™” ì„¸ì…˜ ID
    
    Returns:
        dict: {"text": "ë‹µë³€ë‚´ìš©", "sentiment": "happy|sad|curious|comforting"}
    """
    # Safety Fallback ì‘ë‹µ
    FALLBACK_RESPONSE = {
        "text": "í• ë¨¸ë‹ˆ, ì œê°€ ì˜ ëª» ë“¤ì—ˆì–´ìš”. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”? ë©!",
        "sentiment": "curious"
    }
    
    if gemini_model is None:
        logger.error("Gemini ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return FALLBACK_RESPONSE
    
    try:
        # íšŒìƒ ì¹˜ë£Œ í”„ë¡¬í”„íŠ¸ (JSON ê°•ì œ)
        prompt = f"""ë‹¹ì‹ ì€ ë…¸ì¸ íšŒìƒ ì¹˜ë£Œë¥¼ ë•ëŠ” ì¹œê·¼í•œ AI ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì€ 'ë³µì‹¤ì´'ë¼ëŠ” ì´ë¦„ì˜ ê°•ì•„ì§€ ìºë¦­í„°ì…ë‹ˆë‹¤. 

ì‚¬ìš©ì ë§: {user_text}

ë‹¤ìŒ ê°€ì´ë“œë¼ì¸ì„ ë”°ë¼ ë‹µë³€í•˜ì„¸ìš”:
1. ë”°ëœ»í•˜ê³  ê³µê°í•˜ëŠ” ì–´ì¡°ë¡œ ëŒ€í™”í•˜ì„¸ìš”.
2. ê³¼ê±° ê¸°ì–µì„ ë– ì˜¬ë¦´ ìˆ˜ ìˆëŠ” ì§ˆë¬¸ì„ í¬í•¨í•˜ì„¸ìš”.
3. 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
4. ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”.
5. ê°€ë” "ë©!" ë˜ëŠ” "ì™ˆì™ˆ!"ì„ ë¶™ì—¬ì£¼ì„¸ìš”.

**ì¤‘ìš”: ë°˜ë“œì‹œ ì•„ë˜ì˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.**
{{
  "text": "ë‹µë³€ ë‚´ìš©",
  "sentiment": "happy|sad|curious|excited|nostalgic|comforting"
}}
"""
        
        # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‘ë‹µ ìƒì„± (response_mime_type ì œê±° - ë²„ì „ í˜¸í™˜ì„±)
        response = gemini_model.generate_content(prompt)
        
        # ì‘ë‹µ íŒŒì‹±
        if response and response.text:
            import json
            import re
            
            # JSON ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì²˜ë¦¬)
            response_text = response.text.strip()
            
            # ```json ... ``` ë˜ëŠ” ``` ... ``` íŒ¨í„´ ì œê±°
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(1)
            
            # { ... } íŒ¨í„´ ì¶”ì¶œ
            json_match = re.search(r'\{[^{}]*\}', response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(0)
            
            try:
                ai_reply = json.loads(response_text)
                
                # í•„ìˆ˜ í•„ë“œ í™•ì¸
                if "text" in ai_reply and "sentiment" in ai_reply:
                    logger.info(f"Gemini ë‹µë³€ ì„±ê³µ: {ai_reply['text'][:50]}...")
                    return ai_reply
                else:
                    logger.warning("Gemini ì‘ë‹µì— í•„ìˆ˜ í•„ë“œ ëˆ„ë½, Fallback ì‚¬ìš©")
                    return FALLBACK_RESPONSE
            except json.JSONDecodeError as e:
                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}, ì›ë³¸: {response_text[:100]}")
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì‚¬ìš©
                return {
                    "text": response.text.strip()[:200],
                    "sentiment": "comforting"
                }
        else:
                logger.warning("Gemini ì‘ë‹µì— í•„ìˆ˜ í•„ë“œ ëˆ„ë½, Fallback ì‚¬ìš©")
                return FALLBACK_RESPONSE
    
    except Exception as e:
        logger.error(f"Gemini ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        logger.error(traceback.format_exc())
        return FALLBACK_RESPONSE


# ============================================================
# ê°ì • ë¶„ì„ (ì• ë‹ˆë©”ì´ì…˜ ì—°ë™ìš©)
# ============================================================
def analyze_sentiment(text: str) -> str:
    """
    AI ë‹µë³€ì˜ ê°ì •ì„ ë¶„ì„í•˜ì—¬ ê°•ì•„ì§€ ì• ë‹ˆë©”ì´ì…˜ ê²°ì •
    
    Args:
        text: AI ë‹µë³€ í…ìŠ¤íŠ¸
    
    Returns:
        str: "happy" | "curious" | "nostalgic" | "excited" | "comforting"
    """
    # í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„
    happy_keywords = ["ì¢‹", "ê¸°ë»", "í–‰ë³µ", "ì›ƒ", "ì¬ë°Œ", "ì‹ ë‚˜", "ë©‹ì§€"]
    curious_keywords = ["ë­", "ì–´ë””", "ëˆ„êµ¬", "ì–¸ì œ", "ì™œ", "ì–´ë–»ê²Œ", "?"]
    nostalgic_keywords = ["ì¶”ì–µ", "ì˜›ë‚ ", "ê·¸ë•Œ", "ê¸°ì–µ", "ì˜ˆì „", "ì–´ë¦´", "ì˜¤ë˜"]
    excited_keywords = ["ì™€", "ìš°ì™€", "ëŒ€ë°•", "ì •ë§", "ì§„ì§œ", "!"]
    
    text_lower = text.lower()
    
    # ìš°ì„ ìˆœìœ„: curious > nostalgic > excited > happy > comforting
    for kw in curious_keywords:
        if kw in text_lower:
            return "curious"
    for kw in nostalgic_keywords:
        if kw in text_lower:
            return "nostalgic"
    for kw in excited_keywords:
        if kw in text_lower:
            return "excited"
    for kw in happy_keywords:
        if kw in text_lower:
            return "happy"
    
    return "comforting"  # ê¸°ë³¸ê°’: ë”°ëœ»í•œ ìœ„ë¡œ


# ============================================================
# Celery íƒœìŠ¤í¬: ì´ë¯¸ì§€ ë¶„ì„ (Gemini Vision)
# ============================================================
@celery_app.task(bind=True, name="worker.tasks.analyze_image")
def analyze_image(self: Task, image_path: str, prompt: str):
    """
    ì´ë¯¸ì§€ë¥¼ Gemini Visionìœ¼ë¡œ ë¶„ì„
    
    Args:
        image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
        prompt: ë¶„ì„ ìš”ì²­ í”„ë¡¬í”„íŠ¸
    
    Returns:
        dict: ë¶„ì„ ê²°ê³¼
    """
    try:
        load_models()
        
        if gemini_model is None:
            raise RuntimeError("Gemini ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì´ë¯¸ì§€ ë¡œë”©
        from PIL import Image
        image = Image.open(image_path)
        
        # Gemini Vision ë¶„ì„
        response = gemini_model.generate_content([prompt, image])
        
        return {
            "status": "success",
            "analysis": response.text.strip()
        }
    
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            "status": "error",
            "message": str(e)
        }


# ============================================================
# Celery íƒœìŠ¤í¬: í…ìŠ¤íŠ¸ ì „ìš© ëŒ€í™” (í…ŒìŠ¤íŠ¸ìš©)
# ============================================================
@celery_app.task(bind=True, name="worker.tasks.generate_reply_from_text")
def generate_reply_from_text(self: Task, user_text: str, user_id: str, session_id: str = None):
    """
    í…ìŠ¤íŠ¸ ì…ë ¥ìœ¼ë¡œ AI ë‹µë³€ ìƒì„± + TTS ìŒì„± í•©ì„±
    """
    try:
        load_models()
        
        # Geminië¡œ ë‹µë³€ ìƒì„± (JSON ë°˜í™˜)
        reply_data = generate_reply(user_text, user_id, session_id)
        ai_reply = reply_data["text"]
        sentiment = reply_data["sentiment"]
        logger.info(f"âœ… AI ë‹µë³€ ìƒì„± ì™„ë£Œ: {ai_reply[:50]}... (sentiment={sentiment})")
        
        return {
            "status": "success",
            "user_text": user_text,
            "ai_reply": ai_reply,
            "sentiment": sentiment,
            "session_id": session_id,
            "gemini_response": ai_reply  # í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ í˜¸í™˜
        }
    
    except Exception as e:
        logger.error(f"í…ìŠ¤íŠ¸ ëŒ€í™” ì‹¤íŒ¨: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            "status": "error",
            "message": str(e)
        }


# ============================================================
# Celery íƒœìŠ¤í¬: ì¶”ì–µ ì˜ìƒ ìƒì„±
# ============================================================
@celery_app.task(bind=True, name="worker.tasks.generate_memory_video")
def generate_memory_video(
    self: Task,
    session_id: str,
    video_id: str,
    video_type: str = "slideshow"
):
    """
    ëŒ€í™” ì„¸ì…˜ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì–µ ì˜ìƒ ìƒì„±

    Flow:
    1. SessionPhotoì—ì„œ ì‚¬ì§„ ëª©ë¡ ì¡°íšŒ
    2. ë‚´ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± (Gemini)
    3. TTS ë‚´ë ˆì´ì…˜ ìƒì„± (Qwen3-TTS)
    4. ì˜ìƒ ìƒì„± (slideshow: FFmpeg / ai_animated: Replicate SVD)
    5. S3 ì—…ë¡œë“œ

    Args:
        session_id: ëŒ€í™” ì„¸ì…˜ ID
        video_id: ìƒì„±í•  ì˜ìƒ ID
        video_type: "slideshow" (FFmpeg) ë˜ëŠ” "ai_animated" (Replicate SVD)

    Returns:
        dict: ì˜ìƒ URL ë° ìƒíƒœ
    """
    db = None
    temp_files = []  # ì •ë¦¬í•  ì„ì‹œ íŒŒì¼ë“¤

    try:
        from common.database import SessionLocal
        from common.models import (
            ChatSession, GeneratedVideo, ChatLog, VideoStatus,
            SessionPhoto, VideoType
        )
        from worker.ffmpeg_client import generate_slideshow, get_video_duration
        from common.s3_client import upload_video, download_image

        db = SessionLocal()

        # ì„¸ì…˜ ì¡°íšŒ
        session = db.query(ChatSession).filter(ChatSession.id == session_id).first()
        if not session:
            raise ValueError(f"ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {session_id}")

        # ì˜ìƒ ë ˆì½”ë“œ ì¡°íšŒ
        video = db.query(GeneratedVideo).filter(GeneratedVideo.id == video_id).first()
        if not video:
            raise ValueError(f"ì˜ìƒ ë ˆì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_id}")

        # ìƒíƒœ ì—…ë°ì´íŠ¸: PROCESSING
        video.status = VideoStatus.PROCESSING
        db.commit()

        # ============================================================
        # Step 1: ì‚¬ì§„ ìˆ˜ì§‘
        # ============================================================
        logger.info(f"[ì˜ìƒ ìƒì„±] Step 1: ì‚¬ì§„ ìˆ˜ì§‘ (session_id={session_id})")

        # SessionPhoto í…Œì´ë¸”ì—ì„œ ìˆœì„œëŒ€ë¡œ ì¡°íšŒ
        session_photos = (
            db.query(SessionPhoto)
            .filter(SessionPhoto.session_id == session_id)
            .order_by(SessionPhoto.display_order)
            .all()
        )

        # SessionPhotoê°€ ì—†ìœ¼ë©´ main_photoë¡œ fallback
        if not session_photos:
            if not session.main_photo:
                raise ValueError("ì„¸ì…˜ì— ì‚¬ì§„ì´ ì—†ìŠµë‹ˆë‹¤.")
            photo_records = [type('obj', (object,), {'photo_url': session.main_photo})]
        else:
            photo_records = session_photos

        # ì‚¬ì§„ ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬
        local_photo_paths = []
        for i, photo in enumerate(photo_records):
            try:
                # S3ì—ì„œ ë‹¤ìš´ë¡œë“œ
                photo_url = photo.photo_url
                local_path = f"/app/data/photo_{video_id}_{i}.jpg"
                temp_files.append(local_path)
                
                download_image(photo_url, local_path)
                
                # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (ë¦¬ì‚¬ì´ì¦ˆ, í¬ë§· í†µì¼)
                processed_path = f"/app/data/photo_{video_id}_{i}_processed.jpg"
                temp_files.append(processed_path)
                
                preprocess_image_for_ai(
                    local_path,
                    processed_path,
                    target_size=(1920, 1080),  # Full HD
                    quality=95
                )
                
                local_photo_paths.append(processed_path)
                logger.info(f"   ì‚¬ì§„ {i+1}/{len(photo_records)} ì¤€ë¹„ ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"   ì‚¬ì§„ {i+1} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                continue

        if not local_photo_paths:
            raise ValueError("ì²˜ë¦¬ ê°€ëŠ¥í•œ ì‚¬ì§„ì´ ì—†ìŠµë‹ˆë‹¤.")

        logger.info(f"[ì˜ìƒ ìƒì„±] {len(local_photo_paths)}ì¥ ì‚¬ì§„ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")

        # ============================================================
        # Step 2: ëŒ€í™” ë¡œê·¸ ìˆ˜ì§‘ ë° ë‚´ë ˆì´ì…˜ ìƒì„±
        # ============================================================
        logger.info(f"[ì˜ìƒ ìƒì„±] Step 2: ë‚´ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±")

        logs = db.query(ChatLog).filter(ChatLog.session_id == session_id).all()
        conversation_text = "\n".join([
            f"{'ì‚¬ìš©ì' if log.role == 'user' else 'ê°•ì•„ì§€'}: {log.content}"
            for log in logs
        ])

        if gemini_model is None:
            load_models()

        photo_count = len(local_photo_paths)
        narration_prompt = f"""ë‹¤ìŒì€ í• ë¨¸ë‹ˆì™€ ë°˜ë ¤ê²¬ AIì˜ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤.

{conversation_text}

ì´ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì†ì£¼ê°€ í• ë¨¸ë‹ˆì—ê²Œ ë“¤ë ¤ì£¼ëŠ” ë”°ëœ»í•œ ë‚´ë ˆì´ì…˜**ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
{photo_count}ì¥ì˜ ì‚¬ì§„ì´ ìŠ¬ë¼ì´ë“œì‡¼ë¡œ ë³´ì—¬ì§ˆ ì˜ˆì •ì…ë‹ˆë‹¤.
ì „ì²´ 3-5ë¬¸ì¥ìœ¼ë¡œ ë”°ëœ»í•˜ê³  ê°ë™ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë‚´ë ˆì´ì…˜:"""

        response = gemini_model.generate_content(narration_prompt)
        narration_text = response.text.strip()
        logger.info(f"[ì˜ìƒ ìƒì„±] ë‚´ë ˆì´ì…˜: {narration_text[:100]}...")

        # ============================================================
        # Step 3: ë°°ê²½ìŒì•… ì„¤ì • (TTS ì œê±°ë¨)
        # ============================================================
        logger.info(f"[ì˜ìƒ ìƒì„±] Step 3: ë°°ê²½ìŒì•… ì„¤ì • (TTS ì œê±°)")
        # BGM íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¬´ìŒìœ¼ë¡œ ì²˜ë¦¬
        bgm_path = os.path.join(settings.models_root, "bgm", "emotional_bgm.mp3")
        if os.path.exists(bgm_path):
            narration_audio_path = bgm_path
            logger.info(f"[ì˜ìƒ ìƒì„±] BGM ì‚¬ìš©: {bgm_path}")
        else:
            narration_audio_path = None
            logger.warning(f"[ì˜ìƒ ìƒì„±] BGM íŒŒì¼ ì—†ìŒ, ë¬´ìŒìœ¼ë¡œ ìƒì„±")

        # ============================================================
        # Step 4: ì˜ìƒ ìƒì„±
        # ============================================================
        logger.info(f"[ì˜ìƒ ìƒì„±] Step 4: ì˜ìƒ ë Œë”ë§ (type={video_type})")
        output_video_path = f"/app/data/video_{video_id}.mp4"
        temp_files.append(output_video_path)

        if video_type == "slideshow":
            # FFmpeg ìŠ¬ë¼ì´ë“œì‡¼ ìƒì„±
            generate_slideshow(
                image_paths=local_photo_paths,
                audio_path=narration_audio_path,  # BGM ë˜ëŠ” None
                output_path=output_video_path,
                duration_per_image=4.0  # ì‚¬ì§„ë‹¹ 4ì´ˆ (ì–´ë¥´ì‹  ì‹œì²­ ê³ ë ¤)
            )
        else:
            # Replicate SVD ì• ë‹ˆë©”ì´ì…˜ (ê¸°ì¡´ ë¡œì§)
            from common.replicate_client import generate_animated_video
            
            svd_outputs = []
            for img_path in local_photo_paths:
                video_url = generate_animated_video(img_path)
                svd_outputs.append(video_url)
            
            # ì˜ìƒ ë³‘í•© + ì˜¤ë””ì˜¤ ì¶”ê°€
            from worker.ffmpeg_client import merge_videos_with_audio
            merge_videos_with_audio(
                video_paths=svd_outputs,
                audio_path=narration_audio_path,
                output_path=output_video_path
            )

        # ============================================================
        # Step 5: S3 ì—…ë¡œë“œ
        # ============================================================
        logger.info(f"[ì˜ìƒ ìƒì„±] Step 5: S3 ì—…ë¡œë“œ")

        video_url, thumbnail_url = upload_video(
            output_video_path,
            str(session.user_id),
            str(video_id)
        )

        # ì˜ìƒ ê¸¸ì´ ì¡°íšŒ
        duration = get_video_duration(output_video_path)

        # ============================================================
        # Step 6: DB ì—…ë°ì´íŠ¸
        # ============================================================
        video.video_url = video_url
        video.thumbnail_url = thumbnail_url
        video.status = VideoStatus.COMPLETED
        video.video_type = VideoType(video_type)
        video.duration_seconds = duration
        db.commit()

        logger.info(f"[ì˜ìƒ ìƒì„±] âœ… ì™„ë£Œ: {video_url} ({duration:.1f}ì´ˆ)")

        return {
            "status": "success",
            "video_id": str(video_id),
            "video_url": video_url,
            "thumbnail_url": thumbnail_url,
            "duration_seconds": duration
        }

    except Exception as e:
        logger.error(f"[ì˜ìƒ ìƒì„±] âŒ ì‹¤íŒ¨: {str(e)}")
        logger.error(traceback.format_exc())

        # ìƒíƒœ ì—…ë°ì´íŠ¸: FAILED
        if db and video:
            try:
                video.status = VideoStatus.FAILED
                db.commit()
            except:
                pass

        return {
            "status": "error",
            "message": str(e)
        }

    finally:
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        for temp_file in temp_files:
            try:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
            except:
                pass
        
        if db:
            db.close()