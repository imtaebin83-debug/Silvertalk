"""
Celery AI ì²˜ë¦¬ íƒœìŠ¤í¬
- ë™ì  ë””ë°”ì´ìŠ¤ ê°ì§€ (CUDA/CPU ìë™ ì „í™˜)
- AI ëª¨ë¸: Faster-Whisper (STT), Gemini (LLM/Vision), Coqui XTTS (TTS)
"""
import os
import logging
from celery import Task
from worker.celery_app import celery_app
from common.config import settings
import torch

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================
# ë™ì  ë””ë°”ì´ìŠ¤ ê°ì§€
# ============================================================
def detect_device():
    """
    ì‹¤í–‰ í™˜ê²½ì— ë”°ë¼ ìë™ìœ¼ë¡œ ë””ë°”ì´ìŠ¤ ì„ íƒ
    
    Returns:
        device (str): "cuda" ë˜ëŠ” "cpu"
        compute_type (str): "float16" (GPU) ë˜ëŠ” "int8" (CPU)
    """
    if torch.cuda.is_available():
        device = "cuda"
        compute_type = "float16"
        gpu_name = torch.cuda.get_device_name(0)
        logger.info(f"ğŸš€ GPU ê°ì§€: {gpu_name} - CUDA ëª¨ë“œ í™œì„±í™”")
    else:
        device = "cpu"
        compute_type = "int8"
        logger.info("ğŸ’» GPU ë¯¸ê°ì§€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰ (ë¡œì»¬ ê°œë°œ í™˜ê²½)")
    
    return device, compute_type


# ì „ì—­ ë””ë°”ì´ìŠ¤ ì„¤ì •
DEVICE, COMPUTE_TYPE = detect_device()

# ============================================================
# AI ëª¨ë¸ ì „ì—­ ë³€ìˆ˜ (ì›Œì»¤ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ë¡œë“œ)
# ============================================================

whisper_model = None
tts_model = None
gemini_model = None


# ============================================================
# ëª¨ë¸ ë¡œë”© í•¨ìˆ˜
# ============================================================
def load_models():
    """
    AI ëª¨ë¸ ì´ˆê¸°í™” (ì›Œì»¤ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰)
    
    ì—ëŸ¬ ì²˜ë¦¬:
    - ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì‹œì—ë„ ì›Œì»¤ê°€ ì£½ì§€ ì•Šë„ë¡ try-except ì‚¬ìš©
    """
    global whisper_model, tts_model, gemini_model
    
    # STT: Faster-Whisper ë¡œë”©
    if whisper_model is None:
        try:
            from faster_whisper import WhisperModel
            
            # í™˜ê²½ë³„ ëª¨ë¸ ê²½ë¡œ ìë™ ì„¤ì •
            whisper_root = os.path.join(settings.models_root, "whisper")
            os.makedirs(whisper_root, exist_ok=True)
            
            whisper_model = WhisperModel(
                model_size_or_path="large-v3",  # í•œêµ­ì–´ ì„±ëŠ¥ ìµœìƒ
                device=DEVICE,
                compute_type=COMPUTE_TYPE,
                download_root=whisper_root
            )
            logger.info(f"âœ… Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ (device={DEVICE}, path={whisper_root})")
        except Exception as e:
            logger.error(f"âŒ Whisper ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            whisper_model = None
    
    # TTS: Coqui XTTS v2 ë¡œë”©
    if tts_model is None:
        try:
            from TTS.api import TTS
            
            tts_model = TTS(
                model_name="tts_models/multilingual/multi-dataset/xtts_v2",
                progress_bar=False
            ).to(DEVICE)
            logger.info(f"âœ… XTTS ëª¨ë¸ ë¡œë”© ì™„ë£Œ (device={DEVICE})")
        except Exception as e:
            logger.error(f"âŒ XTTS ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            tts_model = None
    
    # LLM: Gemini 1.5 Flash ì´ˆê¸°í™”
    if gemini_model is None:
        try:
            import google.generativeai as genai
            
            api_key = os.getenv("GEMINI_API_KEY")
            if not api_key:
                raise ValueError("GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            genai.configure(api_key=api_key)
            gemini_model = genai.GenerativeModel("gemini-1.5-flash")
            logger.info("âœ… Gemini 1.5 Flash ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ Gemini ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            gemini_model = None


# ============================================================
# Celery íƒœìŠ¤í¬: ìŒì„± ëŒ€í™” ì²˜ë¦¬ (STT + Brain + TTS)
# ============================================================
@celery_app.task(bind=True, name="worker.tasks.process_audio_and_reply")
def process_audio_and_reply(self: Task, audio_path: str, user_id: str, session_id: str = None):
    """
    ìŒì„± íŒŒì¼ì„ ë°›ì•„ AI ëŒ€í™” ì²˜ë¦¬
    
    Flow:
    1. STT: Faster-Whisperë¡œ ìŒì„± â†’ í…ìŠ¤íŠ¸
    2. Brain: Geminië¡œ ëŒ€í™” ìƒì„±
    3. TTS: XTTSë¡œ í…ìŠ¤íŠ¸ â†’ ìŒì„±
    
    Args:
        audio_path: ì—…ë¡œë“œëœ ìŒì„± íŒŒì¼ ê²½ë¡œ
        user_id: ì‚¬ìš©ì ID
        session_id: ëŒ€í™” ì„¸ì…˜ ID (ì˜µì…˜)
    
    Returns:
        dict: {
            "user_text": ì‚¬ìš©ì ìŒì„± í…ìŠ¤íŠ¸,
            "ai_reply": AI ë‹µë³€ í…ìŠ¤íŠ¸,
            "audio_url": TTS ìƒì„± ìŒì„± íŒŒì¼ ê²½ë¡œ
        }
    """
    try:
        # ëª¨ë¸ ë¡œë”© (ì²« ì‹¤í–‰ ì‹œì—ë§Œ)
        load_models()
        
        # Step 1: STT (ìŒì„± â†’ í…ìŠ¤íŠ¸)
        logger.info(f"[STT] ìŒì„± ì¸ì‹ ì‹œì‘: {audio_path}")
        user_text = transcribe_audio(audio_path)
        logger.info(f"[STT] ì¸ì‹ ê²°ê³¼: {user_text}")
        
        # Step 2: Brain (Geminië¡œ ëŒ€í™” ìƒì„±)
        logger.info(f"[Brain] AI ë‹µë³€ ìƒì„± ì¤‘...")
        ai_reply = generate_reply(user_text, user_id, session_id)
        logger.info(f"[Brain] AI ë‹µë³€: {ai_reply}")
        
        # Step 3: TTS (í…ìŠ¤íŠ¸ â†’ ìŒì„±)
        logger.info(f"[TTS] ìŒì„± í•©ì„± ì‹œì‘...")
        output_audio_path = f"/app/data/{user_id}_reply_{self.request.id}.wav"
        synthesize_speech(ai_reply, output_audio_path)
        logger.info(f"[TTS] ìŒì„± í•©ì„± ì™„ë£Œ: {output_audio_path}")
        
        return {
            "status": "success",
            "user_text": user_text,
            "ai_reply": ai_reply,
            "audio_url": output_audio_path
        }
    
    except Exception as e:
        logger.error(f"âŒ ìŒì„± ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        return {
            "status": "error",
            "message": str(e)
        }


# ============================================================
# STT: Faster-Whisper
# ============================================================
def transcribe_audio(audio_path: str) -> str:
    """
    ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    
    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
    
    Returns:
        str: ì¸ì‹ëœ í…ìŠ¤íŠ¸
    """
    if whisper_model is None:
        raise RuntimeError("Whisper ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        segments, info = whisper_model.transcribe(
            audio_path,
            language="ko",  # í•œêµ­ì–´
            beam_size=5,
            vad_filter=True  # Voice Activity Detection
        )
        
        # ì„¸ê·¸ë¨¼íŠ¸ ê²°í•©
        text = " ".join([segment.text for segment in segments])
        return text.strip()
    
    except Exception as e:
        logger.error(f"STT ì‹¤íŒ¨: {str(e)}")
        return ""


# ============================================================
# Brain: Gemini 1.5 Flash
# ============================================================
def generate_reply(user_text: str, user_id: str, session_id: str = None) -> str:
    """
    ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ AI ë‹µë³€ ìƒì„±
    
    Args:
        user_text: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
        user_id: ì‚¬ìš©ì ID
        session_id: ëŒ€í™” ì„¸ì…˜ ID
    
    Returns:
        str: AI ë‹µë³€
    """
    if gemini_model is None:
        raise RuntimeError("Gemini ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        # íšŒìƒ ì¹˜ë£Œ í”„ë¡¬í”„íŠ¸
        prompt = f"""ë‹¹ì‹ ì€ ë…¸ì¸ íšŒìƒ ì¹˜ë£Œë¥¼ ë•ëŠ” ì¹œê·¼í•œ AI ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ë§: {user_text}

ë‹¤ìŒ ê°€ì´ë“œë¼ì¸ì„ ë”°ë¼ ë‹µë³€í•˜ì„¸ìš”:
1. ë”°ëœ»í•˜ê³  ê³µê°í•˜ëŠ” ì–´ì¡°ë¡œ ëŒ€í™”í•˜ì„¸ìš”.
2. ê³¼ê±° ê¸°ì–µì„ ë– ì˜¬ë¦´ ìˆ˜ ìˆëŠ” ì§ˆë¬¸ì„ í¬í•¨í•˜ì„¸ìš”.
3. 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
4. ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ì„¸ìš”.

ë‹µë³€:"""
        
        response = gemini_model.generate_content(prompt)
        ai_reply = response.text.strip()
        
        return ai_reply
    
    except Exception as e:
        logger.error(f"Gemini ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


# ============================================================
# TTS: Coqui XTTS v2
# ============================================================
def synthesize_speech(text: str, output_path: str, speaker_wav: str = None):
    """
    í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜
    
    Args:
        text: í•©ì„±í•  í…ìŠ¤íŠ¸
        output_path: ì¶œë ¥ ìŒì„± íŒŒì¼ ê²½ë¡œ
        speaker_wav: ëª©ì†Œë¦¬ ë³µì œìš© ìƒ˜í”Œ (ì˜µì…˜)
    """
    if tts_model is None:
        raise RuntimeError("XTTS ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        # ê¸°ë³¸ ëª©ì†Œë¦¬ë¡œ ìƒì„± (ë˜ëŠ” ì»¤ìŠ¤í…€ ëª©ì†Œë¦¬ ì‚¬ìš© ê°€ëŠ¥)
        tts_model.tts_to_file(
            text=text,
            file_path=output_path,
            speaker_wav=speaker_wav,  # Noneì´ë©´ ê¸°ë³¸ ëª©ì†Œë¦¬
            language="ko"
        )
        logger.info(f"TTS ì™„ë£Œ: {output_path}")
    
    except Exception as e:
        logger.error(f"TTS ì‹¤íŒ¨: {str(e)}")
        raise


# ============================================================
# Celery íƒœìŠ¤í¬: ì´ë¯¸ì§€ ë¶„ì„ (Gemini Vision)
# ============================================================
@celery_app.task(bind=True, name="worker.tasks.analyze_image")
def analyze_image(self: Task, image_path: str, prompt: str):
    """
    ì´ë¯¸ì§€ë¥¼ Gemini Visionìœ¼ë¡œ ë¶„ì„
    
    Args:
        image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
        prompt: ë¶„ì„ ìš”ì²­ í”„ë¡¬í”„íŠ¸
    
    Returns:
        dict: ë¶„ì„ ê²°ê³¼
    """
    try:
        load_models()
        
        if gemini_model is None:
            raise RuntimeError("Gemini ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì´ë¯¸ì§€ ë¡œë”©
        from PIL import Image
        image = Image.open(image_path)
        
        # Gemini Vision ë¶„ì„
        response = gemini_model.generate_content([prompt, image])
        
        return {
            "status": "success",
            "analysis": response.text.strip()
        }
    
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
        return {
            "status": "error",
            "message": str(e)
        }


# ============================================================
# Celery íƒœìŠ¤í¬: í…ìŠ¤íŠ¸ ì „ìš© ëŒ€í™” (í…ŒìŠ¤íŠ¸ìš©)
# ============================================================
@celery_app.task(bind=True, name="worker.tasks.generate_reply_from_text")
def generate_reply_from_text(self: Task, user_text: str, user_id: str, session_id: str = None):
    """
    í…ìŠ¤íŠ¸ ì…ë ¥ë§Œìœ¼ë¡œ AI ë‹µë³€ ìƒì„± (STT/TTS ì—†ì´)
    """
    try:
        load_models()
        
        ai_reply = generate_reply(user_text, user_id, session_id)
        
        return {
            "status": "success",
            "user_text": user_text,
            "ai_reply": ai_reply
        }
    
    except Exception as e:
        logger.error(f"í…ìŠ¤íŠ¸ ëŒ€í™” ì‹¤íŒ¨: {str(e)}")
        return {
            "status": "error",
            "message": str(e)
        }


# ============================================================
# Celery íƒœìŠ¤í¬: ì¶”ì–µ ì˜ìƒ ìƒì„±
# ============================================================
@celery_app.task(bind=True, name="worker.tasks.generate_memory_video")
def generate_memory_video(self: Task, session_id: str, video_id: str):
    """
    ëŒ€í™” ì„¸ì…˜ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì–µ ì˜ìƒ ìƒì„±
    
    Flow:
    1. ChatSessionì—ì„œ ëŒ€í™” ìš”ì•½ ì¡°íšŒ
    2. ì‚½í™” ìƒì„± (Replicate/Flux API ë˜ëŠ” ì›ë³¸ ì‚¬ì§„ í™œìš©)
    3. ë‚´ë ˆì´ì…˜ ìƒì„± (Coqui XTTS v2)
    4. FFmpegë¡œ ì˜ìƒ ë Œë”ë§
    5. S3 ì—…ë¡œë“œ
    
    Args:
        session_id: ëŒ€í™” ì„¸ì…˜ ID
        video_id: ìƒì„±í•  ì˜ìƒ ID
    
    Returns:
        dict: ì˜ìƒ URL ë° ìƒíƒœ
    """
    try:
        from common.database import SessionLocal
        from common.models import ChatSession, GeneratedVideo, ChatLog, VideoStatus
        
        db = SessionLocal()
        
        # ì„¸ì…˜ ì¡°íšŒ
        session = db.query(ChatSession).filter(ChatSession.id == session_id).first()
        if not session:
            raise ValueError("ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì˜ìƒ ë ˆì½”ë“œ ì¡°íšŒ
        video = db.query(GeneratedVideo).filter(GeneratedVideo.id == video_id).first()
        if not video:
            raise ValueError("ì˜ìƒ ë ˆì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸: PROCESSING
        video.status = VideoStatus.PROCESSING
        db.commit()
        
        # Step 1: ëŒ€í™” ë¡œê·¸ ìˆ˜ì§‘
        logger.info(f"[ì˜ìƒ ìƒì„±] Step 1: ëŒ€í™” ë¡œê·¸ ìˆ˜ì§‘ (session_id={session_id})")
        logs = db.query(ChatLog).filter(ChatLog.session_id == session_id).all()
        
        conversation_text = "\n".join([
            f"{'ì‚¬ìš©ì' if log.role == 'user' else 'ê°•ì•„ì§€'}: {log.content}"
            for log in logs
        ])
        
        # Step 2: ë‚´ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± (Gemini)
        logger.info(f"[ì˜ìƒ ìƒì„±] Step 2: ë‚´ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±")
        if gemini_model is None:
            load_models()
        
        narration_prompt = f"""ë‹¤ìŒì€ í• ë¨¸ë‹ˆì™€ ë°˜ë ¤ê²¬ AIì˜ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤.

{conversation_text}

ì´ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ì†ì£¼ê°€ í• ë¨¸ë‹ˆì—ê²Œ ë“¤ë ¤ì£¼ëŠ” ë”°ëœ»í•œ ë‚´ë ˆì´ì…˜**ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
2-3ë¬¸ì¥ìœ¼ë¡œ ì§§ê³  ê°ë™ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë‚´ë ˆì´ì…˜:"""
        
        response = gemini_model.generate_content(narration_prompt)
        narration_text = response.text.strip()
        logger.info(f"[ì˜ìƒ ìƒì„±] ë‚´ë ˆì´ì…˜: {narration_text}")
        
        # Step 3: TTS ë‚´ë ˆì´ì…˜ ìƒì„±
        logger.info(f"[ì˜ìƒ ìƒì„±] Step 3: TTS ìŒì„± ìƒì„±")
        narration_audio_path = f"/app/data/video_{video_id}_narration.wav"
        synthesize_speech(narration_text, narration_audio_path)
        
        # Step 4: FFmpegë¡œ ì˜ìƒ ë Œë”ë§ (ê°„ë‹¨í•œ ë²„ì „)
        logger.info(f"[ì˜ìƒ ìƒì„±] Step 4: ì˜ìƒ ë Œë”ë§")
        output_video_path = f"/app/data/video_{video_id}.mp4"
        
        # ì›ë³¸ ì‚¬ì§„ ê²½ë¡œ
        main_photo = session.main_photo
        photo_path = main_photo.s3_url if main_photo else None
        
        if photo_path and photo_path.startswith("http"):
            # S3 URL -> ë¡œì»¬ ë‹¤ìš´ë¡œë“œ (ì¶”í›„ êµ¬í˜„)
            photo_path = "/app/data/placeholder.jpg"
        
        # FFmpeg ëª…ë ¹ì–´ (ì‚¬ì§„ + ì˜¤ë””ì˜¤)
        # (ì¶”í›„ êµ¬í˜„: ffmpeg-python ì‚¬ìš©)
        # í˜„ì¬ëŠ” placeholder
        
        # Step 5: S3 ì—…ë¡œë“œ (ì¶”í›„ êµ¬í˜„)
        logger.info(f"[ì˜ìƒ ìƒì„±] Step 5: S3 ì—…ë¡œë“œ")
        video_url = f"https://s3.amazonaws.com/silvertalk/videos/{video_id}.mp4"
        thumbnail_url = f"https://s3.amazonaws.com/silvertalk/videos/{video_id}_thumb.jpg"
        
        # ì˜ìƒ ë ˆì½”ë“œ ì—…ë°ì´íŠ¸
        video.video_url = video_url
        video.thumbnail_url = thumbnail_url
        video.status = VideoStatus.COMPLETED
        db.commit()
        
        db.close()
        
        logger.info(f"[ì˜ìƒ ìƒì„±] âœ… ì™„ë£Œ: {video_url}")
        
        return {
            "status": "success",
            "video_id": str(video_id),
            "video_url": video_url,
            "thumbnail_url": thumbnail_url
        }
    
    except Exception as e:
        logger.error(f"[ì˜ìƒ ìƒì„±] âŒ ì‹¤íŒ¨: {str(e)}")
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸: FAILED
        try:
            db = SessionLocal()
            video = db.query(GeneratedVideo).filter(GeneratedVideo.id == video_id).first()
            if video:
                video.status = VideoStatus.FAILED
                db.commit()
            db.close()
        except:
            pass
        
        return {
            "status": "error",
            "message": str(e)
        }
